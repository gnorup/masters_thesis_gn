{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "calculate subjects that had missing values even though transcriptions and audio-files were present\n",
    "& determine why they were missing in the first place"
   ],
   "id": "244266b67da7be9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T21:09:45.188979Z",
     "start_time": "2025-08-25T21:09:45.175078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup\n",
    "import os, sys, shutil, datetime, wave, contextlib, traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# project root\n",
    "sys.path.append(\"/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/src\")\n",
    "\n",
    "from config.constants import DATA_DIRECTORY, GIT_DIRECTORY\n",
    "\n",
    "# features\n",
    "from feature_extraction.features import (\n",
    "    n_words, clean_text, tokenize, pos_ratios_spacy, filler_word_ratio,\n",
    "    ttr, mattr, avg_word_length, light_verb_ratio, empty_word_ratio, nid_ratio,\n",
    "    adjacent_repetitions, brunets_index, honores_statistic, guirauds_statistic,\n",
    "    article_pause_contentword\n",
    ")\n",
    "from feature_extraction.features.psycholinguistic_features import (\n",
    "    compute_avg_by_pos, load_aoa_lexicon, load_imageability_norms,\n",
    "    load_familiarity_norms, load_frequency_norms, load_concreteness_lexicon\n",
    ")\n",
    "from feature_extraction.features.fluency_features import calculate_fluency_features\n",
    "from feature_extraction.audio import extract_acoustic_features, extract_egemaps, VoiceActivityDetector\n",
    "\n",
    "# manually add durations (if can't be calculated from .wav file)\n",
    "OVERRIDE_DURATIONS = {\n",
    "    # cookieTheft\n",
    "    (\"56\",   \"cookieTheft\"): 44,\n",
    "    (\"149\",  \"cookieTheft\"): 99,\n",
    "    (\"1009\", \"cookieTheft\"): 141\n",
    "}\n"
   ],
   "id": "d35ed7c47287a5ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup → /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/features/cookieTheft.csv.bak.20250825-230945\n",
      "backup → /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/features/filtered/cookieTheft_filtered.csv.bak.20250825-230945\n",
      "backup → /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/features/journaling.csv.bak.20250825-230945\n",
      "backup → /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/features/filtered/journaling_filtered.csv.bak.20250825-230945\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T20:34:30.137769Z",
     "start_time": "2025-08-25T20:34:30.134215Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "def safe_upsert(existing: pd.DataFrame, new_rows: pd.DataFrame, key=\"Subject_ID\") -> pd.DataFrame:\n",
    "    if new_rows is None or new_rows.empty:\n",
    "        return existing\n",
    "    ex = existing.copy(); nw = new_rows.copy()\n",
    "    ex[key] = ex[key].astype(str); nw[key] = nw[key].astype(str)\n",
    "    for c in nw.columns:\n",
    "        if c not in ex.columns:\n",
    "            ex[c] = pd.NA\n",
    "    ex = ex.set_index(key)\n",
    "    nw = nw.set_index(key)\n",
    "    ex.update(nw)  # only updates overlapping columns for overlapping keys\n",
    "    return ex.reset_index()\n",
    "\n",
    "def _infer_wav_duration_seconds(wav_path: str):\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(wav_path, 'r')) as w:\n",
    "            return w.getnframes() / float(w.getframerate())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_audio_durations(subject_folder, task):\n",
    "    # 1) manual override\n",
    "    subject_id = os.path.basename(subject_folder)\n",
    "    key = (str(subject_id), str(task))\n",
    "    if key in OVERRIDE_DURATIONS:\n",
    "        return float(OVERRIDE_DURATIONS[key])\n",
    "    # 2) subject CSV\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(subject_folder, \"audio_durations.csv\"))\n",
    "        m = df[df[\"task\"] == task]\n",
    "        if not m.empty:\n",
    "            return float(m[\"duration\"].values[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) fallback: infer from WAV\n",
    "    wav_path = os.path.join(subject_folder, f\"{task}.wav\")\n",
    "    return _infer_wav_duration_seconds(wav_path)\n",
    "\n",
    "def load_transcription(subject_folder, task):\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(subject_folder, \"ASR\", \"transcriptions.csv\"))\n",
    "        m = df[df[\"task\"] == task][\"text_google\"]\n",
    "        return m.iloc[0] if not m.empty else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def load_audio_file(subject_folder, task):\n",
    "    p = os.path.join(subject_folder, f\"{task}.wav\")\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "def _diagnose_inputs(subject_folder, task):\n",
    "    issues = []\n",
    "    dur = load_audio_durations(subject_folder, task)\n",
    "    if dur is None: issues.append(\"no audio duration\")\n",
    "    txt = load_transcription(subject_folder, task)\n",
    "    if not txt: issues.append(\"no transcription text\")\n",
    "    wav = load_audio_file(subject_folder, task)\n",
    "    if wav is None: issues.append(\"no audio file\")\n",
    "    return issues, dur, txt, wav\n"
   ],
   "id": "6db1c992b5c6dd9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T20:34:34.415159Z",
     "start_time": "2025-08-25T20:34:34.405315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_features(task, subject_ids=None):\n",
    "    \"\"\"\n",
    "    Compute features for specific task; if subject_ids specified, only process those.\n",
    "    Upserts rows into results/features/{task}.csv.\n",
    "    \"\"\"\n",
    "    base_dir = DATA_DIRECTORY\n",
    "    out_dir  = os.path.join(GIT_DIRECTORY, \"results/features\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_csv  = os.path.join(out_dir, f\"{task}.csv\")\n",
    "\n",
    "    # load lexicons once\n",
    "    concreteness_lexicon = load_concreteness_lexicon()\n",
    "    aoa_lexicon          = load_aoa_lexicon()\n",
    "    frequency_lexicon    = load_frequency_norms()\n",
    "    familiarity_lexicon  = load_familiarity_norms()\n",
    "    imageability_lexicon = load_imageability_norms()\n",
    "\n",
    "    # select subjects\n",
    "    all_subjects = sorted([s for s in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, s)) and s.isdigit()],\n",
    "                          key=lambda x: int(x))\n",
    "    if subject_ids is not None:\n",
    "        keep = {str(s) for s in subject_ids}\n",
    "        subjects = [s for s in all_subjects if s in keep]\n",
    "        missing  = sorted(keep - set(subjects))\n",
    "        if missing: print(f\"[warn] subject folder(s) not found: {missing}\")\n",
    "    else:\n",
    "        subjects = all_subjects\n",
    "\n",
    "    rows, errors = [], {}\n",
    "    for sid in subjects:\n",
    "        subj_folder = os.path.join(base_dir, sid)\n",
    "        print(f\"Calculating features for subject {sid}...\")\n",
    "        issues, duration, text, wav_path = _diagnose_inputs(subj_folder, task)\n",
    "        if issues: print(f\"[diag] {sid} → {', '.join(issues)}\")\n",
    "\n",
    "        text_feats, pos_ratios, acoustic, eg = {}, {}, {}, {}\n",
    "        try:\n",
    "            if text:\n",
    "                # core text features\n",
    "                text_feats[\"n_words\"] = n_words(text)\n",
    "                text_feats[\"ttr\"] = ttr(text)\n",
    "                text_feats.update(mattr(text, window_sizes=[10,20,30,40,50]))\n",
    "                text_feats[\"filler_word_ratio\"] = filler_word_ratio(text)\n",
    "                text_feats[\"average_word_length\"] = avg_word_length(text)\n",
    "                text_feats[\"brunets_index\"] = brunets_index(text)\n",
    "                text_feats[\"honores_statistic\"] = honores_statistic(text)\n",
    "                text_feats[\"guirauds_statistic\"] = guirauds_statistic(text)\n",
    "                text_feats[\"light_verb_ratio\"] = light_verb_ratio(text)\n",
    "                text_feats[\"empty_word_ratio\"]  = empty_word_ratio(text)\n",
    "                text_feats[\"nid_ratio\"] = nid_ratio(text)\n",
    "                text_feats[\"adjacent_repetitions\"] = adjacent_repetitions(text)\n",
    "\n",
    "                # psycholinguistic aggregates\n",
    "                text_feats[\"aoa_content\"]  = compute_avg_by_pos(text, aoa_lexicon, pos_tags=[\"NOUN\",\"VERB\",\"ADJ\"])\n",
    "                text_feats[\"aoa_nouns\"]    = compute_avg_by_pos(text, aoa_lexicon, pos_tags=[\"NOUN\"])\n",
    "                text_feats[\"aoa_verbs\"]    = compute_avg_by_pos(text, aoa_lexicon, pos_tags=[\"VERB\"])\n",
    "\n",
    "                text_feats[\"fam_content\"]  = compute_avg_by_pos(text, familiarity_lexicon, pos_tags=[\"NOUN\",\"VERB\",\"ADJ\"])\n",
    "                text_feats[\"fam_nouns\"]    = compute_avg_by_pos(text, familiarity_lexicon, pos_tags=[\"NOUN\"])\n",
    "                text_feats[\"fam_verbs\"]    = compute_avg_by_pos(text, familiarity_lexicon, pos_tags=[\"VERB\"])\n",
    "\n",
    "                text_feats[\"img_content\"]  = compute_avg_by_pos(text, imageability_lexicon, pos_tags=[\"NOUN\",\"VERB\",\"ADJ\"])\n",
    "                text_feats[\"img_nouns\"]    = compute_avg_by_pos(text, imageability_lexicon, pos_tags=[\"NOUN\"])\n",
    "                text_feats[\"img_verbs\"]    = compute_avg_by_pos(text, imageability_lexicon, pos_tags=[\"VERB\"])\n",
    "\n",
    "                text_feats[\"freq_content\"] = compute_avg_by_pos(text, frequency_lexicon, pos_tags=[\"NOUN\",\"VERB\",\"ADJ\"])\n",
    "                text_feats[\"freq_nouns\"]   = compute_avg_by_pos(text, frequency_lexicon, pos_tags=[\"NOUN\"])\n",
    "                text_feats[\"freq_verbs\"]   = compute_avg_by_pos(text, frequency_lexicon, pos_tags=[\"VERB\"])\n",
    "\n",
    "                text_feats[\"concr_content\"] = compute_avg_by_pos(text, concreteness_lexicon, pos_tags=[\"NOUN\",\"VERB\",\"ADJ\"])\n",
    "                text_feats[\"concr_nouns\"]   = compute_avg_by_pos(text, concreteness_lexicon, pos_tags=[\"NOUN\"])\n",
    "                text_feats[\"concr_verbs\"]   = compute_avg_by_pos(text, concreteness_lexicon, pos_tags=[\"VERB\"])\n",
    "\n",
    "                pos_ratios = pos_ratios_spacy(text)\n",
    "                text_feats.update(calculate_fluency_features(text))\n",
    "\n",
    "            if wav_path and duration:\n",
    "                acoustic = extract_acoustic_features(wav_path, text, duration)\n",
    "                eg       = extract_egemaps(wav_path)\n",
    "\n",
    "            if any(value is not None for value in [*text_feats.values(), *pos_ratios.values(), *acoustic.values(), *eg.values()]):\n",
    "                rows.append({\"Subject_ID\": sid, **text_feats, **pos_ratios, **acoustic, **eg})\n",
    "            else:\n",
    "                print(f\"[skip] {sid}: no features extracted (missing inputs?)\")\n",
    "\n",
    "        except Exception:\n",
    "            errors[sid] = traceback.format_exc()\n",
    "            print(f\"[error] {sid}: exception during feature extraction\")\n",
    "\n",
    "    if errors:\n",
    "        log_path = os.path.join(out_dir, f\"{task}_recompute_errors.log\")\n",
    "        with open(log_path, \"w\") as f:\n",
    "            for s, tb in errors.items(): f.write(f\"--- {s} ---\\n{tb}\\n\\n\")\n",
    "        print(f\"[info] error log written to: {log_path}\")\n",
    "\n",
    "    new_df = pd.DataFrame(rows)\n",
    "    if not new_df.empty: new_df[\"Subject_ID\"] = new_df[\"Subject_ID\"].astype(str)\n",
    "\n",
    "    # safe write\n",
    "    if os.path.exists(out_csv):\n",
    "        old = pd.read_csv(out_csv)\n",
    "        old[\"Subject_ID\"] = old[\"Subject_ID\"].astype(str)\n",
    "        merged = safe_upsert(old, new_df, key=\"Subject_ID\")\n",
    "    else:\n",
    "        merged = new_df\n",
    "\n",
    "    merged.to_csv(out_csv, index=False)\n",
    "    print(f\"[ok] updated {out_csv} (safe upsert)\")\n",
    "\n",
    "def backfill_egemaps_if_missing(df: pd.DataFrame, task: str) -> pd.DataFrame:\n",
    "    eg_cols = [\"eGeMAPS_jitterLocal_sma3nz_amean\", \"eGeMAPS_shimmerLocaldB_sma3nz_amean\"]\n",
    "    out = []\n",
    "    for _, row in df.iterrows():\n",
    "        need = any((c not in row.index) or pd.isna(row.get(c)) for c in eg_cols)\n",
    "        if not need: out.append(row); continue\n",
    "        sid = str(row[\"Subject_ID\"])\n",
    "        wav = os.path.join(DATA_DIRECTORY, sid, f\"{task}.wav\")\n",
    "        if not os.path.exists(wav): out.append(row); continue\n",
    "        try:\n",
    "            eg = extract_egemaps(wav)\n",
    "            if isinstance(eg, dict):\n",
    "                for c in eg_cols:\n",
    "                    if c in eg and (c not in row.index or pd.isna(row.get(c))):\n",
    "                        row[c] = eg[c]\n",
    "        except Exception:\n",
    "            pass\n",
    "        out.append(row)\n",
    "    return pd.DataFrame(out)\n"
   ],
   "id": "1e6fa46af6d12280",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sync_to_clean_and_filtered(task):\n",
    "    src_path      = os.path.join(GIT_DIRECTORY, \"results/features\", f\"{task}.csv\")\n",
    "    cleaned_path  = os.path.join(GIT_DIRECTORY, \"results/features\", f\"{task}_cleaned.csv\")\n",
    "    filtered_path = os.path.join(GIT_DIRECTORY, \"results/features/filtered\", f\"{task}_filtered.csv\")\n",
    "\n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"[warn] missing {src_path}\");\n",
    "        return\n",
    "\n",
    "    src = pd.read_csv(src_path)\n",
    "    if \"Subject_ID\" not in src.columns:\n",
    "        print(\"[warn] Subject_ID missing in source\");\n",
    "        return\n",
    "    src[\"Subject_ID\"] = src[\"Subject_ID\"].astype(str)\n",
    "\n",
    "    if os.path.exists(cleaned_path):\n",
    "        cleaned = pd.read_csv(cleaned_path)\n",
    "        cleaned[\"Subject_ID\"] = cleaned[\"Subject_ID\"].astype(str)\n",
    "        cleaned = safe_upsert(cleaned, src, key=\"Subject_ID\")\n",
    "        cleaned.to_csv(cleaned_path, index=False)\n",
    "        print(f\"synced → {cleaned_path}\")\n",
    "\n",
    "    if os.path.exists(filtered_path):\n",
    "        filt = pd.read_csv(filtered_path)\n",
    "        filt[\"Subject_ID\"] = filt[\"Subject_ID\"].astype(str)\n",
    "        keep_cols = [c for c in src.columns if c in set(filt.columns) | {\"Subject_ID\"}]\n",
    "        filt = safe_upsert(filt, src[keep_cols], key=\"Subject_ID\")\n",
    "        filt.to_csv(filtered_path, index=False)\n",
    "        print(f\"synced → {filtered_path}\")"
   ],
   "id": "f120e01e3afeac38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T20:35:43.946453Z",
     "start_time": "2025-08-25T20:34:47.456582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cookieTheft: 56, 149, 772, 1009\n",
    "print(\"\\nprocessing selected subjects for task: cookieTheft...\\n\")\n",
    "process_features(\"cookieTheft\", subject_ids=[56, 149, 772, 1009])\n",
    "\n",
    "# journaling: 898, 1202\n",
    "print(\"\\nprocessing selected subjects for task: journaling...\\n\")\n",
    "process_features(\"journaling\", subject_ids=[898, 1202])\n",
    "\n",
    "# keep the other CSVs consistent\n",
    "sync_to_clean_and_filtered(\"cookieTheft\")\n",
    "sync_to_clean_and_filtered(\"journaling\")\n"
   ],
   "id": "1e01a43f6711c4e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing selected subjects for task: cookieTheft...\n",
      "\n",
      "Calculating features for subject 56...\n",
      "Calculating features for subject 149...\n",
      "Calculating features for subject 772...\n",
      "Calculating features for subject 1009...\n",
      "updated /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/features/cookieTheft.csv with extracted features (safe upsert)\n",
      "\n",
      "processing selected subjects for task: journaling...\n",
      "\n",
      "Calculating features for subject 898...\n",
      "Calculating features for subject 1202...\n",
      "updated /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/features/journaling.csv with extracted features (safe upsert)\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
