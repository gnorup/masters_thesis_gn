{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "244266b67da7be9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T17:45:14.582556Z",
     "start_time": "2025-10-19T17:44:07.554032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# add project root\n",
    "sys.path.append(\"/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/src\")\n",
    "\n",
    "from config.constants import GIT_DIRECTORY\n",
    "from config.feature_sets import get_linguistic_features, get_acoustic_features\n",
    "from regression.regression_helpers import stratified_cross_validation\n",
    "from regression.model_evaluation_helpers import (\n",
    "    load_task_dataframe, get_model_feature_list, complete_subjects,\n",
    "    normalize_oof_df, bootstrap_summary_from_oof, compare_models_bootstrap,\n",
    "    bootstrap_metrics_from_oof\n",
    ")\n",
    "\n",
    "# parameters \n",
    "tasks = [\"cookieTheft\", \"picnicScene\", \"journaling\"]\n",
    "score_names = [\"PictureNamingScore\",\"SemanticFluencyScore\",\"PhonemicFluencyScore\"]\n",
    "\n",
    "oof_dir = os.path.join(GIT_DIRECTORY, \"results\", \"regression\", \"oof_results_linear\")\n",
    "os.makedirs(oof_dir, exist_ok=True)\n",
    "\n",
    "model_type = LinearRegression\n",
    "model_params = {\"fit_intercept\": True, \"copy_X\": True, \"positive\": False}\n",
    "\n",
    "def main():\n",
    "    scores_df = pd.read_csv(os.path.join(GIT_DIRECTORY, \"data/language_scores_all_subjects.csv\"))\n",
    "    demographics = pd.read_csv(os.path.join(GIT_DIRECTORY, \"data/demographics_data.csv\"))\n",
    "\n",
    "    # prepare demographics \n",
    "    for col in [\"Gender\",\"Education\",\"Country\"]:\n",
    "        demographics[col] = demographics[col].astype(\"string\").str.lower().str.strip()\n",
    "    demographics[\"Socioeconomic\"] = pd.to_numeric(demographics[\"Socioeconomic\"], errors=\"coerce\")\n",
    "    demographics[\"Gender\"] = demographics[\"Gender\"].map({\"f\":0, \"m\":1})\n",
    "    education_map = {\"less_than_highschool\":1,\"high_school\":2,\"vocational\":3,\"bachelor\":4,\"master\":5,\"phd\":6,\"no_answer\":np.nan}\n",
    "    demographics[\"Education\"] = demographics[\"Education\"].map(education_map)\n",
    "    demographics[\"Education_level\"] = demographics[\"Education\"].map({1:0, 2:1, 3:1, 4:2, 5:2, 6:2})\n",
    "    demographics[\"Country\"] = demographics[\"Country\"].map({\"uk\":0,\"usa\":1})\n",
    "    demographics.drop(columns=[\"Language\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # model configs \n",
    "    demographic = [\"Age\",\"Gender\",\"Education_level\",\"Country\",\"Socioeconomic\"]\n",
    "    linguistic = get_linguistic_features()\n",
    "    acoustic = get_acoustic_features()\n",
    "    model_configs = {\n",
    "        \"baseline\": [],\n",
    "        \"demographics\": demographic,\n",
    "        \"acoustic\": sorted(list(acoustic)),\n",
    "        \"linguistic\": sorted(list(linguistic)),\n",
    "        \"linguistic+acoustic\": sorted(list(linguistic|acoustic)),\n",
    "        \"full\": sorted(list(linguistic|acoustic)) + demographic\n",
    "    }\n",
    "\n",
    "    all_oof_rows = []\n",
    "    fold_rows = []\n",
    "\n",
    "    # loop all scores once\n",
    "    for tgt in score_names:\n",
    "        # per-task dataframes for this score\n",
    "        task_dfs = {t: load_task_dataframe(t, tgt, scores_df, demographics) for t in tasks}\n",
    "\n",
    "        # feature-complete intersection of subjects (based on full model)\n",
    "        full_requested = model_configs[\"full\"]\n",
    "        full_cols_by_task = {t: get_model_feature_list(task_dfs[t].columns, full_requested, tgt) for t in tasks}\n",
    "        subject_sets = [complete_subjects(task_dfs[t], full_cols_by_task[t], tgt) for t in tasks]\n",
    "        full_subjects = set.intersection(*subject_sets)\n",
    "        pd.Series(sorted(full_subjects), name=\"Subject_ID\").to_csv(\n",
    "            os.path.join(oof_dir, f\"{tgt}_full_subjects.csv\"), index=False\n",
    "        )\n",
    "        print(f\"{tgt}: intersection of subjects across all tasks: N={len(full_subjects)}\")\n",
    "\n",
    "        # fit all models for this score & each task on fixed subject-pool\n",
    "        for t in tasks:\n",
    "            df_t = task_dfs[t]\n",
    "            df_t = df_t[df_t[\"Subject_ID\"].isin(full_subjects)].copy()\n",
    "\n",
    "            for model_name, selected in model_configs.items():\n",
    "                if model_name == \"baseline\":\n",
    "                    df_use = df_t.dropna(subset=[tgt]).copy()\n",
    "                    X = pd.DataFrame(np.ones((len(df_use), 1)), index=df_use.index, columns=[\"__dummy__\"])\n",
    "                    fcols = [\"__dummy__\"]\n",
    "                    mtype, mparams = DummyRegressor, {\"strategy\": \"mean\"}\n",
    "                else:\n",
    "                    fcols = get_model_feature_list(df_t.columns, selected, tgt)\n",
    "                    if not fcols:\n",
    "                        continue\n",
    "                    df_use = df_t.dropna(subset=[tgt] + fcols).copy()\n",
    "                    if df_use.empty:\n",
    "                        continue\n",
    "                    X = df_use[fcols]\n",
    "                    mtype, mparams = model_type, model_params\n",
    "\n",
    "                model_df = pd.concat([df_use[[\"Subject_ID\",\"fold\"]], X, df_use[tgt].rename(tgt)], axis=1)\n",
    "                print(f\"{t} | {model_name} | N={len(model_df)} | features used={0 if model_name=='baseline' else len(fcols)}\")\n",
    "\n",
    "                # cross-validation\n",
    "                r2_list, rmse_list, mae_list, all_preds = stratified_cross_validation(\n",
    "                    df=model_df,\n",
    "                    fold_column=\"fold\",\n",
    "                    model_type=mtype,\n",
    "                    model_params=mparams,\n",
    "                    target_column=tgt,\n",
    "                    feature_columns=fcols\n",
    "                )\n",
    "\n",
    "                # collect fold metrics\n",
    "                for k, (r2, rmse, mae) in enumerate(zip(r2_list, rmse_list, mae_list)):\n",
    "                    fold_rows.append({\"target\":tgt,\"task\":t,\"model\":model_name,\"fold\":k,\n",
    "                                      \"r2\":r2,\"rmse\":rmse,\"mae\":mae,\"estimator\":mtype.__name__})\n",
    "\n",
    "                # store normalized OOF predictions per subject\n",
    "                all_preds = all_preds.rename(columns={\"y_test\":\"y_true\"})\n",
    "                oof_df = normalize_oof_df(all_preds, target_col=tgt)\n",
    "                oof_df[\"task\"] = t; oof_df[\"model\"] = model_name; oof_df[\"target\"] = tgt\n",
    "                oof_df[\"estimator\"] = mtype.__name__\n",
    "\n",
    "                # merge demographics\n",
    "                cols_keep = [\"Subject_ID\", \"Age\", \"Gender\", \"Education_level\", \"Country\"]\n",
    "                oof_df = oof_df.merge(demographics[cols_keep], on=\"Subject_ID\", how=\"left\")\n",
    "                oof_df[\"Gender_label\"]  = oof_df[\"Gender\"].map({0:\"f\", 1:\"m\"})\n",
    "                oof_df[\"Country_label\"] = oof_df[\"Country\"].map({0:\"uk\", 1:\"usa\"})\n",
    "                oof_df[\"AgeGroup\"] = pd.cut(oof_df[\"Age\"], bins=[-np.inf, 65, 75, np.inf], labels=[\"<65\", \"65â€“75\", \">75\"])\n",
    "\n",
    "                all_oof_rows.append(oof_df)\n",
    "\n",
    "        # per-score summaries (bootstrap)\n",
    "        oof_so_far = pd.concat(all_oof_rows, ignore_index=True)\n",
    "        oof_score  = oof_so_far[oof_so_far[\"target\"] == tgt].copy()\n",
    "\n",
    "        _, summ_df = bootstrap_summary_from_oof(oof_score, group_cols=(\"target\",\"task\",\"model\"), n_boot=1000, ci=0.95, random_state=42)\n",
    "        summ_df.to_csv(os.path.join(oof_dir, f\"bootstrap_summary_{tgt}.csv\"), index=False)\n",
    "\n",
    "        met = bootstrap_metrics_from_oof(oof_score, group_cols=(\"target\",\"task\",\"model\"), n_boot=1000, ci=0.95, random_state=42)\n",
    "        met.to_csv(os.path.join(oof_dir, f\"bootstrap_metrics_{tgt}.csv\"), index=False)\n",
    "\n",
    "        rows = []\n",
    "        for t in tasks:\n",
    "            rows.append(compare_models_bootstrap(oof_score, task=t, target=tgt, n_boot=1000, random_state=42))\n",
    "        pd.concat(rows, ignore_index=True).sort_values([\"task\",\"p_boot\",\"model_a\",\"model_b\"]).to_csv(\n",
    "            os.path.join(oof_dir, f\"pairwise_bootstrap_diffs_{tgt}.csv\"), index=False\n",
    "        )\n",
    "\n",
    "    # store results\n",
    "    oof_all = pd.concat(all_oof_rows, ignore_index=True)\n",
    "    oof_all.to_csv(os.path.join(oof_dir, \"oof_preds_all_scores.csv\"), index=False)\n",
    "    pd.DataFrame(fold_rows).to_csv(os.path.join(oof_dir, \"cv_folds_all_scores.csv\"), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "af24a864f8acff39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PictureNamingScore: intersection of subjects across all tasks: N=959\n",
      "cookieTheft | baseline | N=959 | features used=0\n",
      "cookieTheft | demographics | N=959 | features used=5\n",
      "cookieTheft | acoustic | N=959 | features used=19\n",
      "cookieTheft | linguistic | N=959 | features used=38\n",
      "cookieTheft | linguistic+acoustic | N=959 | features used=57\n",
      "cookieTheft | full | N=959 | features used=62\n",
      "picnicScene | baseline | N=959 | features used=0\n",
      "picnicScene | demographics | N=959 | features used=5\n",
      "picnicScene | acoustic | N=959 | features used=19\n",
      "picnicScene | linguistic | N=959 | features used=36\n",
      "picnicScene | linguistic+acoustic | N=959 | features used=55\n",
      "picnicScene | full | N=959 | features used=60\n",
      "journaling | baseline | N=959 | features used=0\n",
      "journaling | demographics | N=959 | features used=5\n",
      "journaling | acoustic | N=959 | features used=19\n",
      "journaling | linguistic | N=959 | features used=36\n",
      "journaling | linguistic+acoustic | N=959 | features used=55\n",
      "journaling | full | N=959 | features used=60\n",
      "SemanticFluencyScore: intersection of subjects across all tasks: N=970\n",
      "cookieTheft | baseline | N=970 | features used=0\n",
      "cookieTheft | demographics | N=970 | features used=5\n",
      "cookieTheft | acoustic | N=970 | features used=19\n",
      "cookieTheft | linguistic | N=970 | features used=38\n",
      "cookieTheft | linguistic+acoustic | N=970 | features used=57\n",
      "cookieTheft | full | N=970 | features used=62\n",
      "picnicScene | baseline | N=970 | features used=0\n",
      "picnicScene | demographics | N=970 | features used=5\n",
      "picnicScene | acoustic | N=970 | features used=19\n",
      "picnicScene | linguistic | N=970 | features used=36\n",
      "picnicScene | linguistic+acoustic | N=970 | features used=55\n",
      "picnicScene | full | N=970 | features used=60\n",
      "journaling | baseline | N=970 | features used=0\n",
      "journaling | demographics | N=970 | features used=5\n",
      "journaling | acoustic | N=970 | features used=19\n",
      "journaling | linguistic | N=970 | features used=36\n",
      "journaling | linguistic+acoustic | N=970 | features used=55\n",
      "journaling | full | N=970 | features used=60\n",
      "PhonemicFluencyScore: intersection of subjects across all tasks: N=970\n",
      "cookieTheft | baseline | N=970 | features used=0\n",
      "cookieTheft | demographics | N=970 | features used=5\n",
      "cookieTheft | acoustic | N=970 | features used=19\n",
      "cookieTheft | linguistic | N=970 | features used=38\n",
      "cookieTheft | linguistic+acoustic | N=970 | features used=57\n",
      "cookieTheft | full | N=970 | features used=62\n",
      "picnicScene | baseline | N=970 | features used=0\n",
      "picnicScene | demographics | N=970 | features used=5\n",
      "picnicScene | acoustic | N=970 | features used=19\n",
      "picnicScene | linguistic | N=970 | features used=36\n",
      "picnicScene | linguistic+acoustic | N=970 | features used=55\n",
      "picnicScene | full | N=970 | features used=60\n",
      "journaling | baseline | N=970 | features used=0\n",
      "journaling | demographics | N=970 | features used=5\n",
      "journaling | acoustic | N=970 | features used=19\n",
      "journaling | linguistic | N=970 | features used=36\n",
      "journaling | linguistic+acoustic | N=970 | features used=55\n",
      "journaling | full | N=970 | features used=60\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
