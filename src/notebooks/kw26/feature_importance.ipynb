{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "calculate feature-importances using SHAP-values",
   "id": "543170a1d4be1974"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:20:45.286904Z",
     "start_time": "2025-11-04T19:20:45.216042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# add root to sys path\n",
    "sys.path.append(\"/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/src\")\n",
    "from config.constants import GIT_DIRECTORY\n",
    "from config.feature_sets import get_linguistic_features, get_acoustic_features\n",
    "from data_preparation.feature_set_helpers import stratified_cv_feature_importance\n",
    "from regression.plotting_helpers import format_title\n",
    "\n",
    "# parameters\n",
    "task_name = \"picnicScene\"\n",
    "target = \"PictureNamingScore\"\n",
    "folds_path = os.path.join(GIT_DIRECTORY, \"data/stratified_folds.csv\")\n",
    "scores_path = os.path.join(GIT_DIRECTORY, \"data/language_scores_all_subjects.csv\")\n",
    "features_path = os.path.join(GIT_DIRECTORY, f\"results/features/filtered/{task_name}_filtered2.csv\")\n",
    "save_dir = os.path.join(GIT_DIRECTORY, \"results/feature_importance_filtered2\", task_name, target)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(features_path)\n",
    "fold_df = pd.read_csv(folds_path)\n",
    "scores_df = pd.read_csv(scores_path)\n",
    "# extract and encode demographics from fold_df\n",
    "demographics = fold_df[[\"Subject_ID\", \"Gender\", \"Education\", \"Country\", \"Age\", \"Socioeconomic\"]].copy()\n",
    "demographics[\"Socioeconomic\"] = pd.to_numeric(demographics[\"Socioeconomic\"], errors=\"coerce\")\n",
    "# Gender: f = 0, m = 1\n",
    "demographics[\"Gender\"] = demographics[\"Gender\"].map({\"f\": 0, \"m\": 1})\n",
    "# Education: group encoding\n",
    "education_map = {\n",
    "    \"less_than_highschool\": 1,\n",
    "    \"high_school\": 2,\n",
    "    \"vocational\": 3,\n",
    "    \"bachelor\": 4,\n",
    "    \"master\": 5,\n",
    "    \"phd\": 6,\n",
    "    \"no_answer\": np.nan\n",
    "}\n",
    "demographics[\"Education\"] = demographics[\"Education\"].map(education_map)\n",
    "demographics[\"Education_level\"] = demographics[\"Education\"].map({\n",
    "    1: 0,            # low\n",
    "    2: 1, 3: 1,      # medium\n",
    "    4: 2, 5: 2, 6: 2 # high\n",
    "})\n",
    "# Country: uk = 0, usa = 1\n",
    "demographics[\"Country\"] = demographics[\"Country\"].map({\"uk\": 0, \"usa\": 1})\n",
    "\n",
    "# merge all data\n",
    "df = pd.merge(df, demographics, on=\"Subject_ID\", how=\"left\")\n",
    "df = pd.merge(df, fold_df[[\"Subject_ID\", \"fold\"]], on=\"Subject_ID\", how=\"left\")\n",
    "df = pd.merge(df, scores_df[[\"Subject_ID\", target]], on=\"Subject_ID\", how=\"left\")\n",
    "feature_cols = [c for c in df.columns if c not in [\"Subject_ID\", \"fold\", target, \"Education\"]]\n",
    "df = df.dropna(subset=[target] + feature_cols)\n",
    "\n",
    "# parameters for random forest\n",
    "rf_params={\"n_estimators\": 625, \"random_state\": 42, \"min_samples_leaf\": 4, \"max_features\": \"sqrt\", \"bootstrap\": True, \"max_depth\": 14, \"min_samples_split\": 9}"
   ],
   "id": "41f2bae5a9eb33d3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:20:48.574603Z",
     "start_time": "2025-11-04T19:20:48.570983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define categories for features\n",
    "\n",
    "linguistic_features = get_linguistic_features()\n",
    "acoustic_features = get_acoustic_features()\n",
    "\n",
    "demographic_features = [\"Age\", \"Gender\", \"Education_level\", \"Country\", \"Socioeconomic\"]\n",
    "\n",
    "def feature_category(name: str) -> str:\n",
    "    if name in linguistic_features:\n",
    "        return \"linguistic\"\n",
    "    if name in acoustic_features:\n",
    "        return \"acoustic\"\n",
    "    if name in demographic_features:\n",
    "        return \"demographics\"\n",
    "    return \"other\"\n",
    "\n",
    "superscript_for = {\n",
    "    \"linguistic\": \"¹\",\n",
    "    \"acoustic\": \"²\",\n",
    "    \"demographics\": \"³\"\n",
    "}"
   ],
   "id": "e26b798f699cc27f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:20:51.190374Z",
     "start_time": "2025-11-04T19:20:51.187435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot style\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams.update({\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.labelcolor\": \"black\",\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"savefig.bbox\": \"tight\"\n",
    "})"
   ],
   "id": "90139b4a85aaaa55",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T19:21:49.727218Z",
     "start_time": "2025-11-04T19:20:53.405190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run CV + importances\n",
    "shap_explanation, shap_table = stratified_cv_feature_importance(\n",
    "    df=df,\n",
    "    fold_column=\"fold\",\n",
    "    model_type=RandomForestRegressor,\n",
    "    model_params=rf_params,\n",
    "    target_column=target,\n",
    "    feature_columns=feature_cols,\n",
    "    save_dir=save_dir,\n",
    "    task_name=task_name\n",
    ")"
   ],
   "id": "7d44e55e9bed7377",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:38:34.009786Z",
     "start_time": "2025-11-05T10:38:33.271838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SHAP beeswarm plot with superscripts and legend\n",
    "\n",
    "orig_expl = shap_explanation\n",
    "orig_names = list(orig_expl.feature_names)\n",
    "\n",
    "name_to_cat = {n: feature_category(n) for n in orig_names}\n",
    "name_to_label = {n: f\"{n}{superscript_for[name_to_cat[n]]}\" for n in orig_names}\n",
    "\n",
    "shap_expl_labeled = shap.Explanation(\n",
    "    values=orig_expl.values,\n",
    "    base_values=orig_expl.base_values,\n",
    "    data=orig_expl.data,\n",
    "    feature_names=[name_to_label[n] for n in orig_names]\n",
    ")\n",
    "\n",
    "shap.plots.beeswarm(shap_expl_labeled, max_display=20, show=False)\n",
    "# add footnote\n",
    "footnote = \"¹ linguistic   ² acoustic   ³ demographics\"\n",
    "plt.figtext(0.0, 0.03, footnote,\n",
    "            ha=\"left\", va=\"top\", fontsize=10, fontfamily=\"Arial\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 0.85, 1])\n",
    "plt.savefig(os.path.join(save_dir, f\"{task_name}_{target}_shap_beeswarm_20.png\"),\n",
    "            dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()"
   ],
   "id": "b8f81f49da6b9c4e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:33:00.324477Z",
     "start_time": "2025-09-02T13:33:00.322099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SHAP plots\n",
    "# shap.plots.bar(shap_explanation, max_display=20, show=False)\n",
    "# plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "# plt.savefig(os.path.join(save_dir, f\"{task_name}_{target}_shap_bar.png\"), dpi=300)\n",
    "# plt.close()\n",
    "#\n",
    "# shap.summary_plot(shap_explanation, plot_type=\"bar\", max_display=20,show=False)\n",
    "# plt.savefig(os.path.join(save_dir, f\"{task_name}_{target}_shap_summary.png\"), dpi=300)\n",
    "# plt.close()\n",
    "\n",
    "# shap.summary_plot(shap_explanation, max_display=20, plot_type=\"violin\", show=False)\n",
    "# plt.savefig(os.path.join(save_dir, f\"{task_name}_{target}_shap_violin.png\"), dpi=300)\n",
    "# plt.close()\n",
    "#\n",
    "# shap.plots.beeswarm(shap_explanation, max_display=20, show=False)\n",
    "# plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "# plt.savefig(os.path.join(save_dir, f\"{task_name}_{target}_shap_beeswarm.png\"), dpi=300)\n",
    "# plt.close()"
   ],
   "id": "55fceba3fe178ba3",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T12:56:49.714728Z",
     "start_time": "2025-10-31T12:56:47.568075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Waterfall plot (local SHAP values one subject)\n",
    "\n",
    "# choose subject & fit model for that subject's fold\n",
    "rng = np.random.default_rng(0)\n",
    "rand_idx = int(rng.integers(0, len(df)))\n",
    "subject_id = int(df.iloc[rand_idx][\"Subject_ID\"])\n",
    "subject_fold = int(df.iloc[rand_idx][\"fold\"])\n",
    "\n",
    "train_df = df[df[\"fold\"] != subject_fold].copy()\n",
    "test_row = df[df[\"Subject_ID\"] == subject_id].copy()\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target]\n",
    "X_test_one = test_row[feature_cols]\n",
    "y_test_one = test_row[target].iloc[0]\n",
    "\n",
    "rf = RandomForestRegressor(**rf_params)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# SHAP explainer\n",
    "background = shap.sample(X_train, min(200, len(X_train)), random_state=42)\n",
    "explainer = shap.TreeExplainer(\n",
    "    rf,\n",
    "    data=background,\n",
    "    feature_perturbation=\"interventional\",\n",
    "    model_output=\"raw\"\n",
    ")\n",
    "\n",
    "# local explanation for the chosen subject\n",
    "ex = explainer(X_test_one)\n",
    "\n",
    "# label features with superscripts\n",
    "orig_names = list(ex.feature_names)\n",
    "name_to_cat = {n: feature_category(n) for n in orig_names}\n",
    "name_to_label = {n: f\"{n}{superscript_for[name_to_cat[n]]}\" for n in orig_names}\n",
    "\n",
    "ex_labeled = shap.Explanation(\n",
    "    values=ex.values,\n",
    "    base_values=ex.base_values,\n",
    "    data=ex.data,\n",
    "    feature_names=[name_to_label[n] for n in orig_names]\n",
    ")\n",
    "\n",
    "# plot: waterfall\n",
    "subject_id_fmt = f\"{subject_id:d}\"\n",
    "title = f\"Local SHAP Values: {format_title(target)} (Subject {subject_id_fmt}, {task_name})\"\n",
    "\n",
    "plt.figure(figsize=(9.5, 6))\n",
    "shap.plots.waterfall(ex_labeled[0], max_display=9, show=False)\n",
    "plt.title(title)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot spacing\n",
    "plt.subplots_adjust(left=0.30, right=0.96, bottom=0.15, top=0.88)\n",
    "\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ax.set_xlim(xmin - 0.10 * (xmax - xmin), xmax + 0.15 * (xmax - xmin))\n",
    "\n",
    "for t in list(ax.texts):\n",
    "    t.set_clip_on(False)\n",
    "\n",
    "for t in list(ax.texts):\n",
    "    s = t.get_text() or \"\"\n",
    "    if \"f(x\" in s or \"E[f\" in s or \"$f(x)$\" in s or \"$E[f(X)]$\" in s:\n",
    "        t.remove()\n",
    "\n",
    "# footnote\n",
    "plt.figtext(0.00, 0.02, \"¹ linguistic   ² acoustic   ³ demographics\",\n",
    "            ha=\"left\", va=\"top\", fontsize=10, fontfamily=\"Arial\")\n",
    "\n",
    "# save\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "out_path = os.path.join(save_dir, f\"{task_name}_{target}_local_waterfall_subject-{subject_id_fmt}.png\")\n",
    "plt.savefig(out_path, dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"Saved: {out_path}\")\n",
    "\n"
   ],
   "id": "aa101eba7c7c4392",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/feature_importance_filtered2/picnicScene/SemanticFluencyScore/picnicScene_SemanticFluencyScore_local_waterfall_subject-1174.png\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T14:53:59.252863Z",
     "start_time": "2025-11-02T14:44:19.922379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add heatmap\n",
    "\n",
    "# SHAP heatmap across scores & tasks \n",
    "import os, sys, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from matplotlib.colors import TwoSlopeNorm  \n",
    "\n",
    "# project paths\n",
    "sys.path.append(\"/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/src\")\n",
    "from config.constants import GIT_DIRECTORY\n",
    "from data_preparation.feature_set_helpers import stratified_cv_feature_importance\n",
    "from config.feature_sets import get_linguistic_features, get_acoustic_features\n",
    "\n",
    "# config \n",
    "TASKS   = [\"cookieTheft\",\"picnicScene\",\"journaling\"]\n",
    "SCORES  = [\"PictureNamingScore\",\"SemanticFluencyScore\",\"PhonemicFluencyScore\"]\n",
    "ORDERED_COLS = [\n",
    "    (\"PictureNamingScore\",\"cookieTheft\"), (\"PictureNamingScore\",\"picnicScene\"), (\"PictureNamingScore\",\"journaling\"),\n",
    "    (\"SemanticFluencyScore\",\"cookieTheft\"), (\"SemanticFluencyScore\",\"picnicScene\"), (\"SemanticFluencyScore\",\"journaling\"),\n",
    "    (\"PhonemicFluencyScore\",\"cookieTheft\"), (\"PhonemicFluencyScore\",\"picnicScene\"), (\"PhonemicFluencyScore\",\"journaling\"),\n",
    "]\n",
    "FEATURES_DIR_TMPL = os.path.join(GIT_DIRECTORY, \"results\", \"features\", \"filtered\", \"{task}_filtered2.csv\")\n",
    "FOLDS_PATH  = os.path.join(GIT_DIRECTORY, \"data\", \"stratified_folds.csv\")\n",
    "SCORES_PATH = os.path.join(GIT_DIRECTORY, \"data\", \"language_scores_all_subjects.csv\")\n",
    "\n",
    "rf_params = {\"n_estimators\":625, \"random_state\":42, \"min_samples_leaf\":4, \"max_features\":\"sqrt\",\n",
    "             \"bootstrap\":True, \"max_depth\":14, \"min_samples_split\":9}\n",
    "\n",
    "# output \n",
    "OUT_DIR = os.path.join(GIT_DIRECTORY, \"results\", \"feature_importance_filtered2\", \"shap_heatmap\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "CSV_PATH = os.path.join(OUT_DIR, \"mean_shap_heatmap_RF.csv\")\n",
    "PNG_PATH = os.path.join(OUT_DIR, \"mean_shap_heatmap_RF.png\")\n",
    "\n",
    "# helpers \n",
    "def _prep_task_df(task, target):\n",
    "    \"\"\"Load filtered features, merge folds + target; mirror your setup (no changes).\"\"\"\n",
    "    df = pd.read_csv(FEATURES_DIR_TMPL.format(task=task))\n",
    "    folds = pd.read_csv(FOLDS_PATH)\n",
    "    scores = pd.read_csv(SCORES_PATH)\n",
    "\n",
    "    # demographics (as in your code)\n",
    "    demographics = folds[[\"Subject_ID\",\"Gender\",\"Education\",\"Country\",\"Age\",\"Socioeconomic\"]].copy()\n",
    "    demographics[\"Socioeconomic\"] = pd.to_numeric(demographics[\"Socioeconomic\"], errors=\"coerce\")\n",
    "    demographics[\"Gender\"] = demographics[\"Gender\"].map({\"f\":0,\"m\":1})\n",
    "    education_map = {\"less_than_highschool\":1,\"high_school\":2,\"vocational\":3,\"bachelor\":4,\"master\":5,\"phd\":6,\"no_answer\":np.nan}\n",
    "    demographics[\"Education\"] = demographics[\"Education\"].map(education_map)\n",
    "    demographics[\"Education_level\"] = demographics[\"Education\"].map({1:0, 2:1, 3:1, 4:2, 5:2, 6:2})\n",
    "    demographics[\"Country\"] = demographics[\"Country\"].map({\"uk\":0,\"usa\":1})\n",
    "\n",
    "    # merge\n",
    "    df = df.merge(demographics, on=\"Subject_ID\", how=\"left\")\n",
    "    df = df.merge(folds[[\"Subject_ID\",\"fold\"]], on=\"Subject_ID\", how=\"left\")\n",
    "    df = df.merge(scores[[\"Subject_ID\", target]], on=\"Subject_ID\", how=\"left\")\n",
    "\n",
    "    # feature columns \n",
    "    feature_cols = [c for c in df.columns if c not in [\"Subject_ID\",\"fold\",target,\"Education\"]]\n",
    "    df = df.dropna(subset=[target] + feature_cols)\n",
    "    return df, feature_cols\n",
    "\n",
    "def _extract_mean_shap(shap_explanation, shap_table):\n",
    "    \"\"\"Return mean absolute SHAP per feature (global importance, directionless).\"\"\"\n",
    "    vals = np.asarray(shap_explanation.values)  # [n_samples, n_features]\n",
    "    names = list(shap_explanation.feature_names)\n",
    "    mean_abs = np.nanmean(np.abs(vals), axis=0)\n",
    "    return pd.Series(mean_abs, index=names)\n",
    "\n",
    "# compute matrix \n",
    "all_series = {}\n",
    "for score in SCORES:\n",
    "    for task in TASKS:\n",
    "        df, feature_cols = _prep_task_df(task, score)\n",
    "\n",
    "        # run your existing CV + importance function\n",
    "        shap_expl, shap_table = stratified_cv_feature_importance(\n",
    "            df=df,\n",
    "            fold_column=\"fold\",\n",
    "            model_type=RandomForestRegressor,\n",
    "            model_params=rf_params,\n",
    "            target_column=score,\n",
    "            feature_columns=feature_cols,\n",
    "            save_dir=None,\n",
    "            task_name=task\n",
    "        )\n",
    "        s = _extract_mean_shap(shap_expl, shap_table)\n",
    "        s = s.reindex(feature_cols).dropna()\n",
    "        all_series[(score, task)] = s\n",
    "\n",
    "# union of all features\n",
    "all_features = sorted(set().union(*[s.index for s in all_series.values()]))\n",
    "\n",
    "# build MultiIndex columns in requested order\n",
    "cols = pd.MultiIndex.from_tuples(ORDERED_COLS, names=[\"Score\",\"Task\"])\n",
    "mat = pd.DataFrame(index=all_features, columns=cols, dtype=float)\n",
    "\n",
    "for (score, task), s in all_series.items():\n",
    "    if (score, task) in mat.columns:\n",
    "        mat[(score, task)].loc[s.index] = s.values\n",
    "\n",
    "# sort rows by overall absolute mean (values are already >=0)\n",
    "mat[\"__absmean__\"] = mat.mean(axis=1, skipna=True)\n",
    "mat = mat.sort_values(\"__absmean__\", ascending=False).drop(columns=\"__absmean__\")\n",
    "\n",
    "# save CSV (MultiIndex header preserved)\n",
    "mat.to_csv(CSV_PATH, float_format=\"%.3f\")\n",
    "print(\"Saved table to:\", CSV_PATH)\n",
    "\n",
    "# plot heatmap \n",
    "flat_cols = [f\"{sc}\\n{ta}\" for (sc, ta) in mat.columns.to_list()]\n",
    "plot_df = mat.copy()\n",
    "plot_df.columns = flat_cols\n",
    "\n",
    "# add superscripts to feature labels \n",
    "linguistic_features = get_linguistic_features()\n",
    "acoustic_features = get_acoustic_features()\n",
    "demographic_features = [\"Age\",\"Gender\",\"Education_level\",\"Country\",\"Socioeconomic\"]\n",
    "\n",
    "def feature_category(name):\n",
    "    if name in linguistic_features:\n",
    "        return \"¹\"\n",
    "    if name in acoustic_features:\n",
    "        return \"²\"\n",
    "    if name in demographic_features:\n",
    "        return \"³\"\n",
    "    return \"\"\n",
    "\n",
    "plot_df.index = [f\"{f}{feature_category(f)}\" for f in plot_df.index]\n",
    "\n",
    "# sequential color scale from 0 → high |SHAP|\n",
    "vmax = np.nanpercentile(plot_df.values.astype(float), 99)\n",
    "\n",
    "plt.figure(figsize=(14, max(6, 0.28*len(plot_df))))\n",
    "plot_df = plot_df.astype(float)\n",
    "ax = sns.heatmap(\n",
    "    plot_df,\n",
    "    cmap=\"Reds\",            # sequential; no sign\n",
    "    vmin=0.0,\n",
    "    vmax=float(vmax),\n",
    "    linewidths=0.2,\n",
    "    linecolor=\"white\",\n",
    "    cbar_kws={\"label\": \"Mean |SHAP|\"},\n",
    "    square=False,\n",
    "    mask=plot_df.isna(),     # hide NaNs (white)\n",
    ")\n",
    "\n",
    "# clean white background\n",
    "plt.gcf().patch.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# move labels to top\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(rotation=45, ha='left')\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(\"Mean |SHAP| Values by Score × Task (Random Forest)\", pad=50)\n",
    "\n",
    "# footnote: keep at bottom, but tuck it closer to the plot\n",
    "ax.text(\n",
    "    0.0, -0.035,\n",
    "    \"¹ linguistic   ² acoustic   ³ demographics\",\n",
    "    transform=ax.transAxes,\n",
    "    ha=\"left\", va=\"top\", fontsize=10, fontfamily=\"Arial\"\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.savefig(PNG_PATH, dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved heatmap to:\", PNG_PATH)\n"
   ],
   "id": "7cb2a335a38ad7d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 194/196 [00:12<00:00]        /var/folders/py/1h2n_yxd7l9dts6tjm4v1tpr0000gn/T/ipykernel_2586/2869964842.py:100: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  mat[(score, task)].loc[s.index] = s.values\n",
      "/var/folders/py/1h2n_yxd7l9dts6tjm4v1tpr0000gn/T/ipykernel_2586/2869964842.py:104: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  mat = mat.sort_values(\"__absmean__\", ascending=False).drop(columns=\"__absmean__\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved table to: /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/feature_importance_filtered2/shap_heatmap/mean_shap_heatmap_RF.csv\n",
      "Saved heatmap to: /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/feature_importance_filtered2/shap_heatmap/mean_shap_heatmap_RF.png\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
