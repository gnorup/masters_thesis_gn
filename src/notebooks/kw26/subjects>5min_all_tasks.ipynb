{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T13:25:48.270382Z",
     "start_time": "2025-06-28T13:25:48.217963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/gilanorup/Desktop/Studium/MSc/MA/dataset/combined_audio_durations.csv\")\n",
    "\n",
    "# print what subjects talk for more than 5 minutes\n",
    "\n",
    "def print_long_speakers(df, task_name, threshold=300):\n",
    "    long_speakers = df[df[task_name] > threshold]\n",
    "    print(f\"\\n{task_name} subjects with >5min ({len(long_speakers)} total):\")\n",
    "    print(long_speakers[[\"Subject_ID\", task_name]])\n",
    "\n",
    "for task in [\"cookieTheft\", \"picnicScene\", \"journaling\"]:\n",
    "    print_long_speakers(df, task)\n"
   ],
   "id": "ca678fb9bcc2d60c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cookieTheft subjects with >5min (1 total):\n",
      "      Subject_ID  cookieTheft\n",
      "1002        1370       3587.4\n",
      "\n",
      "picnicScene subjects with >5min (8 total):\n",
      "      Subject_ID  picnicScene\n",
      "36           122   321.429333\n",
      "411          612   421.362667\n",
      "530          757   422.220000\n",
      "700          971   334.560000\n",
      "772         1059   340.680000\n",
      "825         1133   332.040000\n",
      "903         1237   330.420000\n",
      "1002        1370  1403.520000\n",
      "\n",
      "journaling subjects with >5min (10 total):\n",
      "      Subject_ID  journaling\n",
      "126          242  448.080000\n",
      "303          473  302.760000\n",
      "343          523  422.890667\n",
      "366          554  325.560000\n",
      "536          764  328.852625\n",
      "585          824  334.980000\n",
      "695          966  334.770667\n",
      "854         1175  346.980000\n",
      "951         1304  376.980000\n",
      "1002        1370  492.120000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T13:25:55.343832Z",
     "start_time": "2025-06-28T13:25:48.286927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup\n",
    "\n",
    "import sys\n",
    "\n",
    "# add the root of the project to the path\n",
    "sys.path.append(\"/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/src\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from feature_extraction.features import (\n",
    "    n_words, clean_text, tokenize, pos_ratios_spacy, filler_word_ratio,\n",
    "    ttr, mattr, avg_word_length,\n",
    "    light_verb_ratio, empty_word_ratio, nid_ratio, adjacent_repetitions,\n",
    "    brunets_index, honores_statistic, guirauds_statistic\n",
    ")\n",
    "\n",
    "from feature_extraction.features.psycholinguistic_features import (\n",
    "    compute_avg_by_pos, load_aoa_lexicon, load_imageability_norms,\n",
    "    load_familiarity_norms, load_frequency_norms, load_concreteness_lexicon\n",
    ")\n",
    "from feature_extraction.features.fluency_features import filled_pause_ratio, calculate_fluency_features\n",
    "\n",
    "from feature_extraction.audio import (\n",
    "    count_phonemes, extract_acoustic_features,\n",
    "    extract_egemaps, VoiceActivityDetector\n",
    ")\n",
    "\n",
    "from config.constants import DATA_DIRECTORY\n",
    "from config.constants import GIT_DIRECTORY"
   ],
   "id": "553bef5979b0d53b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/gilanorup/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T13:25:55.350650Z",
     "start_time": "2025-06-28T13:25:55.347831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trim audio\n",
    "\n",
    "def trim_audio(input_path, output_path, duration_sec=300):\n",
    "    audio = AudioSegment.from_wav(input_path)\n",
    "    trimmed = audio[:duration_sec * 1000] # bc pydub uses ms\n",
    "    trimmed.export(output_path, format=\"wav\")\n",
    "    return len(trimmed) / 1000 # return duration in seconds\n"
   ],
   "id": "f1fdaffb54c31eb4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T13:25:55.365607Z",
     "start_time": "2025-06-28T13:25:55.359578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute features for subjects for only five minutes of the audio-file\n",
    "\n",
    "def calculate_features_exception(text, audio_path, total_duration, Subject_ID):\n",
    "\n",
    "    concreteness_lexicon = load_concreteness_lexicon()\n",
    "    aoa_lexicon = load_aoa_lexicon()\n",
    "    frequency_lexicon = load_frequency_norms()\n",
    "    familiarity_lexicon = load_familiarity_norms()\n",
    "    imageability_lexicon = load_imageability_norms()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # linguistic features\n",
    "    features[\"n_words\"] = n_words(text)\n",
    "    features[\"ttr\"] = ttr(text)\n",
    "    features.update(mattr(text, window_sizes=[10, 20, 30, 40, 50]))\n",
    "    features[\"filler_word_ratio\"] = filler_word_ratio(text)\n",
    "    features[\"average_word_length\"] = avg_word_length(text)\n",
    "    features[\"brunets_index\"] = brunets_index(text)\n",
    "    features[\"honores_statistic\"] = honores_statistic(text)\n",
    "    features[\"guirauds_statistic\"] = guirauds_statistic(text)\n",
    "    features[\"light_verb_ratio\"] = light_verb_ratio(text)\n",
    "    features[\"empty_word_ratio\"] = empty_word_ratio(text)\n",
    "    features[\"nid_ratio\"] = nid_ratio(text)\n",
    "    features[\"adjacent_repetitions\"] = adjacent_repetitions(text)\n",
    "    features[\"aoa_content\"] = compute_avg_by_pos(text, aoa_lexicon, [\"NOUN\", \"VERB\", \"ADJ\"])\n",
    "    features[\"aoa_nouns\"] = compute_avg_by_pos(text, aoa_lexicon, [\"NOUN\"])\n",
    "    features[\"aoa_verbs\"] = compute_avg_by_pos(text, aoa_lexicon, [\"VERB\"])\n",
    "    features[\"fam_content\"] = compute_avg_by_pos(text, familiarity_lexicon, [\"NOUN\", \"VERB\", \"ADJ\"])\n",
    "    features[\"fam_nouns\"] = compute_avg_by_pos(text, familiarity_lexicon, [\"NOUN\"])\n",
    "    features[\"fam_verbs\"] = compute_avg_by_pos(text, familiarity_lexicon, [\"VERB\"])\n",
    "    features[\"img_content\"] = compute_avg_by_pos(text, imageability_lexicon, [\"NOUN\", \"VERB\", \"ADJ\"])\n",
    "    features[\"img_nouns\"] = compute_avg_by_pos(text, imageability_lexicon, [\"NOUN\"])\n",
    "    features[\"img_verbs\"] = compute_avg_by_pos(text, imageability_lexicon, [\"VERB\"])\n",
    "    features[\"freq_content\"] = compute_avg_by_pos(text, frequency_lexicon, [\"NOUN\", \"VERB\", \"ADJ\"])\n",
    "    features[\"freq_nouns\"] = compute_avg_by_pos(text, frequency_lexicon, [\"NOUN\"])\n",
    "    features[\"freq_verbs\"] = compute_avg_by_pos(text, frequency_lexicon, [\"VERB\"])\n",
    "    features[\"concr_content\"] = compute_avg_by_pos(text, concreteness_lexicon, [\"NOUN\", \"VERB\", \"ADJ\"])\n",
    "    features[\"concr_nouns\"] = compute_avg_by_pos(text, concreteness_lexicon, [\"NOUN\"])\n",
    "    features[\"concr_verbs\"] = compute_avg_by_pos(text, concreteness_lexicon, [\"VERB\"])\n",
    "    features.update(pos_ratios_spacy(text))\n",
    "    features.update(calculate_fluency_features(text))\n",
    "\n",
    "    # acoustic features\n",
    "    acoustic = extract_acoustic_features(audio_path, text, total_duration)\n",
    "    egemaps = extract_egemaps(audio_path)\n",
    "    features.update(acoustic)\n",
    "    features.update(egemaps)\n",
    "\n",
    "    return pd.DataFrame([{**{\"Subject_ID\": Subject_ID}, **features}])"
   ],
   "id": "63160ff7a061a885",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T13:42:11.089697Z",
     "start_time": "2025-06-28T13:35:44.397559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loop through all shortened transcriptions and run trimming & feature replacement\n",
    "\n",
    "shortened_df = pd.read_csv(os.path.join(GIT_DIRECTORY, \"data\", \"shortened_transcriptions.csv\"))\n",
    "\n",
    "for idx, row in shortened_df.iterrows():\n",
    "    Subject_ID = int(row[\"Subject_ID\"])\n",
    "    task_name = row[\"task\"]\n",
    "    shortened_transcription = row[\"transcription\"]\n",
    "\n",
    "    print(f\"\\nprocessing subject {Subject_ID} ({task_name})\")\n",
    "\n",
    "    # define original and trimmed paths\n",
    "    original_audio_path = os.path.join(DATA_DIRECTORY, str(Subject_ID), f\"{task_name}.wav\")\n",
    "    trimmed_audio_path = os.path.join(GIT_DIRECTORY, \"data\", f\"{Subject_ID}_{task_name}_trimmed.wav\")\n",
    "\n",
    "    # trim audio\n",
    "    total_duration = trim_audio(original_audio_path, trimmed_audio_path, duration_sec=300)\n",
    "\n",
    "    # calculate features\n",
    "    df_cut = calculate_features_exception(shortened_transcription, trimmed_audio_path, total_duration, Subject_ID)\n",
    "\n",
    "    # update in respective feature-set .csv\n",
    "    features_path = os.path.join(GIT_DIRECTORY, \"results\", \"features\", f\"{task_name}.csv\")\n",
    "    df = pd.read_csv(features_path)\n",
    "    df = df[df[\"Subject_ID\"] != Subject_ID]\n",
    "    df = pd.concat([df, df_cut], ignore_index=True).sort_values(\"Subject_ID\")\n",
    "    df.to_csv(features_path, index=False)\n",
    "    print(f\"updated features for subject {Subject_ID} ({task_name})\")\n"
   ],
   "id": "8715cbe385e6c93f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing subject 122 (picnicScene)\n",
      "updated features for subject 122 (picnicScene)\n",
      "\n",
      "processing subject 242 (journaling)\n",
      "updated features for subject 242 (journaling)\n",
      "\n",
      "processing subject 473 (journaling)\n",
      "updated features for subject 473 (journaling)\n",
      "\n",
      "processing subject 523 (journaling)\n",
      "updated features for subject 523 (journaling)\n",
      "\n",
      "processing subject 554 (journaling)\n",
      "updated features for subject 554 (journaling)\n",
      "\n",
      "processing subject 612 (picnicScene)\n",
      "updated features for subject 612 (picnicScene)\n",
      "\n",
      "processing subject 757 (picnicScene)\n",
      "updated features for subject 757 (picnicScene)\n",
      "\n",
      "processing subject 764 (journaling)\n",
      "updated features for subject 764 (journaling)\n",
      "\n",
      "processing subject 824 (journaling)\n",
      "updated features for subject 824 (journaling)\n",
      "\n",
      "processing subject 966 (journaling)\n",
      "updated features for subject 966 (journaling)\n",
      "\n",
      "processing subject 971 (picnicScene)\n",
      "updated features for subject 971 (picnicScene)\n",
      "\n",
      "processing subject 1059 (picnicScene)\n",
      "updated features for subject 1059 (picnicScene)\n",
      "\n",
      "processing subject 1133 (picnicScene)\n",
      "updated features for subject 1133 (picnicScene)\n",
      "\n",
      "processing subject 1175 (journaling)\n",
      "updated features for subject 1175 (journaling)\n",
      "\n",
      "processing subject 1237 (picnicScene)\n",
      "updated features for subject 1237 (picnicScene)\n",
      "\n",
      "processing subject 1304 (journaling)\n",
      "updated features for subject 1304 (journaling)\n",
      "\n",
      "processing subject 1370 (cookieTheft)\n",
      "updated features for subject 1370 (cookieTheft)\n",
      "\n",
      "processing subject 1370 (picnicScene)\n",
      "updated features for subject 1370 (picnicScene)\n",
      "\n",
      "processing subject 1370 (journaling)\n",
      "updated features for subject 1370 (journaling)\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
