{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "12fee4170afb9ebf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.371725Z",
     "start_time": "2025-11-12T10:56:05.341493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup\n",
    "import os, sys, json, itertools, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import shap\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "# project root\n",
    "sys.path.append(\"/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/src\")\n",
    "\n",
    "from config.constants import GIT_DIRECTORY\n",
    "from config.feature_sets import get_linguistic_features, get_acoustic_features\n",
    "from regression.model_evaluation_helpers import confidence_intervals, bootstrap_index_generator, load_demographics, load_task_dataframe, get_model_feature_list, build_feature_sets\n",
    "\n",
    "# config\n",
    "scores_csv = os.path.join(GIT_DIRECTORY, \"data\", \"language_scores_all_subjects.csv\")\n",
    "scores = pd.read_csv(scores_csv)\n",
    "demographics_csv = os.path.join(GIT_DIRECTORY, \"data\", \"demographics_data.csv\")\n",
    "demographics = load_demographics(demographics_csv)\n",
    "\n",
    "task = \"picnicScene\"\n",
    "target = \"PhonemicFluencyScore\"\n",
    "model = \"full\"\n",
    "demographic_variables = [\"Age\",\"Gender\",\"Education_level\",\"Country\",\"Socioeconomic\"]\n",
    "\n",
    "random_state = 42\n",
    "n_boot = 1000\n",
    "ci = 0.95\n",
    "baseline_threshold = 0.5\n",
    "default_parameters = dict(n_estimators=500, random_state=random_state, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "out_dir = os.path.join(GIT_DIRECTORY, \"results\", \"classification\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "base_models = [\"demographics\", \"acoustic\", \"linguistic\", \"acoustic+linguistic\", \"full\"]\n",
    "palette = sns.color_palette(\"Set2\", n_colors=len(base_models))\n",
    "colors = {\n",
    "    m: matplotlib.colors.to_hex(c)\n",
    "    for m, c in zip(base_models, palette[::-1])\n",
    "}\n",
    "\n",
    "# plot style\n",
    "plt.style.use(\"default\")\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"savefig.facecolor\": \"white\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.grid\": False,\n",
    "})\n",
    "plt.rcParams.update({\"savefig.dpi\": 600, \"savefig.bbox\": \"tight\"})\n"
   ],
   "id": "4a380bab4f18ef55",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.384084Z",
     "start_time": "2025-11-12T10:56:05.378176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate norms\n",
    "\n",
    "# compute norms for scores (M, SD & z-scores) -> define low (z-score <-1), middle (z-score -1;1), high performers (z-score >1)\n",
    "def build_population_norms(scores_df, score_cols):\n",
    "    rows = []\n",
    "    for score in score_cols:\n",
    "        s = scores_df[score].dropna().astype(float)\n",
    "        n = int(s.shape[0])\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        mean = float(s.mean())\n",
    "        std  = float(s.std(ddof=1))\n",
    "\n",
    "        z = (s - mean) / std\n",
    "        low_mask = z < -1\n",
    "        low_n = int(low_mask.sum()) # count how many people are in low category\n",
    "\n",
    "        rows.append({\n",
    "            \"score\": score,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"low_threshold\": mean - std,\n",
    "            \"high_threshold\": mean + std,\n",
    "            \"n\": n,\n",
    "            \"low_n\": low_n,\n",
    "            \"not_low_n\": n - low_n,\n",
    "            \"low_prop\": (low_n / n),\n",
    "            \"not_low_prop\": 1 - (low_n / n)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"score\").reset_index(drop=True)\n",
    "\n",
    "# score_cols = [\"PictureNamingScore\", \"SemanticFluencyScore\", \"PhonemicFluencyScore\"]\n",
    "# norms_df = build_population_norms(scores, score_cols)\n",
    "# norms_df.to_csv(os.path.join(GIT_DIRECTORY, \"results\", \"classification\", \"population_norms.csv\"), index=False)\n",
    "# print(norms_df)\n",
    "\n",
    "# load precomputed population norms\n",
    "def load_population_norms(path: str) -> dict:\n",
    "    norms_df = pd.read_csv(path)\n",
    "    norms = {}\n",
    "    for _, r in norms_df.iterrows():\n",
    "        norms[str(r[\"score\"])] = {\n",
    "            \"mean\": float(r[\"mean\"]),\n",
    "            \"std\": float(r[\"std\"]),\n",
    "            \"low_threshold\": float(r[\"low_threshold\"]),\n",
    "            \"high_threshold\": float(r[\"high_threshold\"]),\n",
    "            \"n\": int(r[\"n\"]),\n",
    "        }\n",
    "    return norms\n",
    "\n",
    "norms_path = os.path.join(GIT_DIRECTORY, \"results\", \"classification\", \"population_norms.csv\")\n",
    "population_norms = load_population_norms(norms_path)\n"
   ],
   "id": "a23cd2a7819d2795",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.426910Z",
     "start_time": "2025-11-12T10:56:05.402979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helpers\n",
    "\n",
    "# compare subjects to norms\n",
    "def compute_norms_and_labels(df: pd.DataFrame, target_score: str):\n",
    "    # compare each individual's score to saved population norms\n",
    "    if target_score not in population_norms:\n",
    "        raise ValueError(f\"no population norm found for {target_score} in {norms_path}\")\n",
    "    mean = population_norms[target_score][\"mean\"]\n",
    "    std  = population_norms[target_score][\"std\"]\n",
    "\n",
    "    # z-score vs population mean/SD\n",
    "    z = (df[target_score] - mean) / std\n",
    "    df[f\"{target_score}_z\"] = z\n",
    "\n",
    "    # binary label: low (< -1 SD) vs not_low\n",
    "    df[\"is_low\"] = (z < -1).astype(int)\n",
    "\n",
    "    return df, float(mean), float(std)\n",
    "\n",
    "# after randomized search and grid search: load best parameter set\n",
    "def load_tuned_rf_params():\n",
    "    tuned_path = os.path.join(out_dir, \"grid_search\", \"best_params_final.csv\")\n",
    "    if os.path.exists(tuned_path):\n",
    "        try:\n",
    "            df = pd.read_csv(tuned_path)\n",
    "            params = df.iloc[0].to_dict() # convert parameters from csv to dictionary\n",
    "            params[\"class_weight\"] = \"balanced\"\n",
    "            params[\"random_state\"] = 42\n",
    "            params[\"n_jobs\"] = -1\n",
    "            return params\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read tuned params, using defaults. Error:\", e)\n",
    "    return dict(default_parameters)\n",
    "\n",
    "# calculate metrics to evaluate random forest classifier performance\n",
    "def classification_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob) # predicted probability for positive class (low)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    auc  = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) == 2 else np.nan\n",
    "    sens = recall_score(y_true, y_pred, pos_label=1, zero_division=0) # recall -> TPR (positive / low = class 1)\n",
    "    spec = recall_score(y_true, y_pred, pos_label=0, zero_division=0) # recall -> TNR (negative / middle-high = class 0)\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc  = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    prec = (tp / (tp + fp)) if (tp + fp) > 0 else np.nan\n",
    "    npv  = (tn / (tn + fn)) if (tn + fn) > 0 else np.nan\n",
    "    bal_acc = 0.5 * (sens + spec)\n",
    "\n",
    "    return {\n",
    "        \"auroc\": auc,\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"npv\": npv,\n",
    "        \"f1\": f1,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "        \"threshold\": threshold\n",
    "    }\n",
    "\n",
    "# find best threshold for sensitivity-specificity using youden's index\n",
    "def find_best_threshold_youden(y_true, y_prob):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob) # false positive rate & true positive rate (sensitivity)\n",
    "    specificity = 1 - fpr\n",
    "    J = tpr + specificity - 1 # calculate youden's index\n",
    "    idx = np.argmax(J) # find index of maximum J value -> where combination of sensitivity and specificity is best\n",
    "    best_thr = thresholds[idx] # find corresponding threshold\n",
    "    return {\n",
    "        \"best_threshold\": float(best_thr),\n",
    "        \"sensitivity\": float(tpr[idx]), # at that threshold\n",
    "        \"specificity\": float(specificity[idx]), # at that threshold\n",
    "        \"J\": float(J[idx]) # youden's index\n",
    "    }\n",
    "\n",
    "# bootstrap classification metrics (mean + CI)\n",
    "def bootstrap_classification_metrics(oof_df, threshold=baseline_threshold, n_boot=n_boot, ci=ci, random_state=random_state):\n",
    "    y = oof_df[\"y_true\"].to_numpy() # true labels\n",
    "    p = oof_df[\"y_prob\"].to_numpy() # predicted probabilities\n",
    "    n = len(y)\n",
    "\n",
    "    idxs = bootstrap_index_generator(n, n_boot, random_state)\n",
    "    rows = []\n",
    "    auprc_list = []\n",
    "\n",
    "    for b in range(n_boot): # create bootstrap sample of y & p\n",
    "        idx = idxs[b]\n",
    "        yb, pb = y[idx], p[idx]\n",
    "        rows.append(classification_metrics(yb, pb, threshold=threshold))\n",
    "        auprc_list.append(average_precision_score(yb, pb) if np.unique(yb).size == 2 else np.nan)\n",
    "\n",
    "    boot = pd.DataFrame(rows)\n",
    "    boot[\"auprc\"] = auprc_list\n",
    "\n",
    "    out = {}\n",
    "    for m in [\"auroc\",\"auprc\",\"sensitivity\",\"specificity\",\"accuracy\",\"precision\",\"npv\",\"f1\",\"balanced_accuracy\", \"mcc\"]: # compute metrics on bootstrapped sample\n",
    "        lo, hi = confidence_intervals(boot[m].to_numpy(), ci=ci)\n",
    "        out[f\"{m}_mean\"] = float(np.nanmean(boot[m]))\n",
    "        out[f\"{m}_ci_low\"] = float(lo)\n",
    "        out[f\"{m}_ci_high\"] = float(hi)\n",
    "\n",
    "    out[\"threshold\"] = threshold\n",
    "    return boot, pd.DataFrame([out])\n",
    "\n",
    "# plot ROC-curve (bootstrapped, with CI) with mean AUROC\n",
    "def plot_roc_curve(oof_df, save_path, n_boot=n_boot, random_state=random_state):\n",
    "    y = oof_df[\"y_true\"].to_numpy()\n",
    "    p = oof_df[\"y_prob\"].to_numpy()\n",
    "    n = len(y)\n",
    "    fpr_grid = np.linspace(0, 1, 101)\n",
    "\n",
    "    # bootstrap sampling\n",
    "    idxs = bootstrap_index_generator(n, n_boot, random_state)\n",
    "    tprs = np.empty((n_boot, fpr_grid.size), dtype=float)\n",
    "    aucs = np.empty(n_boot, dtype=float)\n",
    "    for b in range(n_boot):\n",
    "        idx = idxs[b]\n",
    "        fpr_b, tpr_b, _ = roc_curve(y[idx], p[idx])\n",
    "        aucs[b] = roc_auc_score(y[idx], p[idx]) if len(np.unique(y[idx])) == 2 else np.nan\n",
    "        tprs[b] = np.interp(fpr_grid, fpr_b, tpr_b, left=0.0, right=1.0)\n",
    "\n",
    "    # mean TPR (ROC curve) with CI\n",
    "    tpr_mean = np.nanmean(tprs, axis=0)\n",
    "    tpr_lo   = np.nanquantile(tprs, 0.025, axis=0)\n",
    "    tpr_hi   = np.nanquantile(tprs, 0.975, axis=0)\n",
    "\n",
    "    # mean AUC\n",
    "    auc_mean = np.nanmean(aucs)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr_grid, tpr_mean, label=f\"Mean ROC (bootstrapped)\", color=\"C0\")\n",
    "    plt.fill_between(fpr_grid, tpr_lo, tpr_hi, alpha=0.2, label=\"95% CI\", color=\"C0\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\", linewidth=1, color=\"orange\", label=\"Chance\")\n",
    "    plt.plot([], [], ' ', label=f\"AUROC = {auc_mean:.3f}\")\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize=12)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, fancybox=True, framealpha=0.9)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# plot confusion matrix for classifier's predictions\n",
    "def plot_confusion_matrix(oof_df, save_path):\n",
    "    # use labels produced by clf.predict\n",
    "    y_true = oof_df[\"y_true\"].to_numpy().astype(int)\n",
    "    y_pred = oof_df[\"y_pred\"].to_numpy().astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_display = cm  # raw counts instead of percentages\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n",
    "    im = ax.imshow(cm_display, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "\n",
    "    # colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_ylabel(\"Count\", rotation=270, labelpad=12)\n",
    "\n",
    "    ax.set_xticks([0, 1]); ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([\"Middle-High (0)\", \"Low (1)\"])\n",
    "    ax.set_yticklabels([\"Middle-High (0)\", \"Low (1)\"])\n",
    "    ax.set_ylabel(\"True\", fontsize=12); ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    # annotations on top of matrix (white when too dark)\n",
    "    vmax = cm_display.max() if cm_display.size else 0\n",
    "    thresh = vmax / 2.0 if vmax > 0 else 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            color = \"white\" if cm_display[i, j] > thresh else \"black\"\n",
    "            ax.text(j, i, f\"{cm_display[i, j]:.0f}\", ha=\"center\", va=\"center\", color=color, fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "# find threshold that maximizes F1 on precision-recall-curve\n",
    "def find_best_threshold_f1(y_true, y_prob):\n",
    "    # compute precision-recall curve from all possible thresholds, calculate f1 score\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob, pos_label=1)\n",
    "    t = np.append(t, 1.0)\n",
    "    f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n",
    "    # find threshold that maximizes F1\n",
    "    idx = int(np.nanargmax(f1))\n",
    "    return {\n",
    "        \"best_threshold\": float(t[idx]),\n",
    "        \"precision\": float(p[idx]),\n",
    "        \"recall\": float(r[idx]),\n",
    "        \"f1\": float(f1[idx]),\n",
    "    }\n",
    "\n",
    "# find threshold to achieve certain target recall\n",
    "def find_threshold_for_target_recall(y_true, y_prob, target_recall=0.80):\n",
    "    # compute precision-recall curve from all possible thresholds, calculate f1 score\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob, pos_label=1)\n",
    "    t = np.append(t, 1.0)\n",
    "    # indices where recall >= target\n",
    "    idxs = np.where(r >= target_recall)[0]\n",
    "    if len(idxs) == 0:\n",
    "        # pick closest recall if none reach target\n",
    "        idx = int(np.argmin(np.abs(r - target_recall)))\n",
    "    else:\n",
    "        idx = int(idxs[-1])  # highest threshold that still meets recall\n",
    "    return {\n",
    "        \"threshold\": float(t[idx]),\n",
    "        \"precision\": float(p[idx]),\n",
    "        \"recall\": float(r[idx]),\n",
    "    }\n",
    "\n",
    "# plot precision-recall curve\n",
    "def plot_pr_curve(oof_df, save_path):\n",
    "    y_true = oof_df[\"y_true\"].to_numpy().astype(int)\n",
    "    y_prob = oof_df[\"y_prob\"].to_numpy().astype(float)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob, pos_label=1)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    prev = y_true.mean()\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.step(recall, precision, where=\"post\", label=\"Precision-Recall curve\")\n",
    "    plt.hlines(prev, 0, 1, linestyles=\"--\", linewidth=1,\n",
    "               label=f\"Baseline (prevalence={prev:.2f})\")\n",
    "    plt.plot([], [], ' ', label=f\"AUPRC = {ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\", fontsize=12)\n",
    "    plt.ylabel(\"Precision\", fontsize=12)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# plot confusion matrix at other threshold\n",
    "def plot_confusion_matrix_at_threshold(oof_df, threshold, save_path, normalize=False):\n",
    "    y_true = oof_df[\"y_true\"].to_numpy().astype(int)\n",
    "    y_pred = (oof_df[\"y_prob\"].to_numpy().astype(float) >= float(threshold)).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    if normalize:\n",
    "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "            cm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_ylabel(\"Count\" if not normalize else \"Proportion\", rotation=270, labelpad=12)\n",
    "\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels([\"Middle-High (0)\", \"Low (1)\"])\n",
    "    ax.set_yticklabels([\"Middle-High (0)\", \"Low (1)\"])\n",
    "    ax.set_ylabel(\"True\", fontsize=12); ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "\n",
    "    vmax = np.nanmax(cm) if cm.size else 0\n",
    "    thresh = vmax/2.0 if vmax>0 else 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = cm[i, j]\n",
    "            txt = f\"{val:.2f}\" if normalize else f\"{int(val)}\"\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\",\n",
    "                    color=(\"white\" if val > thresh else \"black\"), fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "# bootstrap probability-threshold at specific recall target & bootstrap metrics at that threshold\n",
    "def bootstrap_recall_target(oof_df, target_recall=0.80, n_boot=n_boot, ci=ci, random_state=random_state):\n",
    "    y = oof_df[\"y_true\"].to_numpy().astype(int)\n",
    "    p = oof_df[\"y_prob\"].to_numpy().astype(float)\n",
    "    n = len(y)\n",
    "\n",
    "    idxs = bootstrap_index_generator(n, n_boot, random_state)\n",
    "    rows = []\n",
    "\n",
    "    for b in range(n_boot):\n",
    "        idx = idxs[b]\n",
    "        yb, pb = y[idx], p[idx]\n",
    "\n",
    "        # skip resamples with only one class (cannot compute PR)\n",
    "        if np.unique(yb).size < 2:\n",
    "            continue\n",
    "\n",
    "        prec, rec, thr = precision_recall_curve(yb, pb, pos_label=1)\n",
    "        thr = np.append(thr, 1.0)  # len(thr) = len(rec) - 1\n",
    "\n",
    "        meets = np.where(rec >= target_recall)[0]\n",
    "        j = int(meets[-1]) if len(meets) else int(np.argmin(np.abs(rec - target_recall)))\n",
    "\n",
    "        thr_b = float(thr[j])\n",
    "        met = classification_metrics(yb, pb, threshold=thr_b)\n",
    "        met.update({\n",
    "            \"threshold\": thr_b,\n",
    "            \"precision_at_target\": float(prec[j]),\n",
    "            \"recall_at_target\": float(rec[j])\n",
    "        })\n",
    "        rows.append(met)\n",
    "\n",
    "    boot_df = pd.DataFrame(rows)\n",
    "\n",
    "    # bootstrapped mean + CI\n",
    "    summary = {}\n",
    "    for col in [\n",
    "        \"threshold\", \"precision_at_target\", \"recall_at_target\",\n",
    "        \"accuracy\", \"sensitivity\", \"specificity\", \"precision\",\n",
    "        \"f1\", \"balanced_accuracy\", \"mcc\", \"auroc\"\n",
    "    ]:\n",
    "        if col in boot_df:\n",
    "            arr = boot_df[col].to_numpy()\n",
    "            mean = np.nanmean(arr)\n",
    "            lo, hi = confidence_intervals(arr, ci=ci)\n",
    "        else:\n",
    "            mean, lo, hi = np.nan, np.nan, np.nan\n",
    "\n",
    "        summary[f\"{col}_mean\"] = mean\n",
    "        summary[f\"{col}_ci_low\"] = lo\n",
    "        summary[f\"{col}_ci_high\"] = hi\n",
    "\n",
    "    summary[\"target_recall\"] = float(target_recall)\n",
    "    summary_df = pd.DataFrame([summary])\n",
    "    return boot_df, summary_df\n",
    "\n",
    "# apply bootstrapping to finding the best thresholds\n",
    "def bootstrap_all_thresholds(oof_df, youden_thr, f1_thr, rec80_thr, n_boot=n_boot, ci=ci, random_state=random_state, recall_target=0.80, bootstrap_threshold=\"reselect\"  # \"fixed\" or \"reselect\"\n",
    "):\n",
    "    # fixed: bootstrap metrics at threshold found on full predictions\n",
    "    # reselect: re-pick threshold inside each bootstrap\n",
    "\n",
    "    pieces_raw   = []\n",
    "    pieces_summ  = []\n",
    "\n",
    "    # fixed thresholds (baseline, Youden-J, max-F1, recall80-fixed)\n",
    "    thr_list = [\n",
    "        (\"baseline_0.50\", 0.50),\n",
    "        (\"youdenJ\", youden_thr),\n",
    "        (\"max_f1\", f1_thr),\n",
    "        (\"recall80_fixed\", rec80_thr)\n",
    "    ]\n",
    "\n",
    "    for name, thr in thr_list:\n",
    "        boot_raw, boot_summ = bootstrap_classification_metrics(\n",
    "            oof_df, threshold=thr, n_boot=n_boot, ci=ci, random_state=random_state\n",
    "        )\n",
    "        boot_raw = boot_raw.assign(threshold_type=name)\n",
    "        boot_summ = boot_summ.assign(threshold_type=name)\n",
    "        pieces_raw.append(boot_raw)\n",
    "        pieces_summ.append(boot_summ)\n",
    "\n",
    "    # recall target with reselection per bootstrap\n",
    "    if bootstrap_threshold == \"reselect\":\n",
    "        boot_rec_raw, boot_rec_summ = bootstrap_recall_target(\n",
    "            oof_df, target_recall=recall_target, n_boot=n_boot, ci=ci, random_state=random_state\n",
    "        )\n",
    "        boot_rec_raw  = boot_rec_raw.assign(threshold_type=f\"recall>={recall_target:.2f}_reselect\")\n",
    "        boot_rec_summ = boot_rec_summ.assign(threshold_type=f\"recall>={recall_target:.2f}_reselect\")\n",
    "        pieces_raw.append(boot_rec_raw)\n",
    "        pieces_summ.append(boot_rec_summ)\n",
    "\n",
    "    all_raw  = pd.concat(pieces_raw, ignore_index=True)\n",
    "    all_summ = pd.concat(pieces_summ, ignore_index=True)\n",
    "\n",
    "    # order columns\n",
    "    metric_cols = [\"auroc\",\"auprc\",\"sensitivity\",\"specificity\",\"accuracy\",\"precision\",\"npv\",\"f1\",\"balanced_accuracy\",\"mcc\",\"threshold\"]\n",
    "    summ_cols = [c for c in all_summ.columns if c.endswith(\"_mean\") or c.endswith(\"_ci_low\") or c.endswith(\"_ci_high\")]\n",
    "    ordered = [\"threshold_type\"] + sorted(set(all_summ.columns) - set(summ_cols) - {\"threshold_type\"}) + sorted(summ_cols)\n",
    "    all_summ = all_summ[ordered]\n",
    "    return all_raw, all_summ\n"
   ],
   "id": "cc412f68fbb3afd7",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.440993Z",
     "start_time": "2025-11-12T10:56:05.429573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SHAP helpers\n",
    "\n",
    "# normalize to a single-class explanation\n",
    "def to_single_output_explanation(expl, class_index=1):\n",
    "    vals = np.array(expl.values)\n",
    "    base = np.array(expl.base_values)\n",
    "    data = np.array(expl.data)\n",
    "\n",
    "    # (n_samples, n_features) -> already single-output (binary)\n",
    "    if vals.ndim == 2:\n",
    "        single = (vals.shape[0] == 1)\n",
    "        return shap.Explanation(\n",
    "            values=vals[0, :] if single else vals,\n",
    "            # only flatten base when single sample; otherwise keep original shape\n",
    "            base_values=float(np.ravel(base)[0]) if (single and base.size >= 1) else base,\n",
    "            # only flatten data when single sample\n",
    "            data=data[0, :] if (data.ndim >= 2 and data.shape[0] == 1) else data,\n",
    "            feature_names=expl.feature_names\n",
    "        )\n",
    "\n",
    "    # (n_features,) -> single sample collapsed to 1D\n",
    "    if vals.ndim == 1:\n",
    "        b = float(np.ravel(base)[0]) if base.size >= 1 else base\n",
    "        d = data if data.ndim == 1 else data[0, :]\n",
    "        return shap.Explanation(values=vals, base_values=b, data=d, feature_names=expl.feature_names)\n",
    "\n",
    "    # (n_samples, n_features, n_outputs) -> pick class_index\n",
    "    if vals.ndim == 3:\n",
    "        k = class_index if vals.shape[2] > class_index else 0\n",
    "        vals_sel = vals[:, :, k]\n",
    "        base_sel = base[:, k] if base.ndim == 2 else (base[k] if base.ndim == 1 else base)\n",
    "        single = (vals_sel.shape[0] == 1)\n",
    "        data_sel = data\n",
    "        return shap.Explanation(\n",
    "            values=vals_sel[0, :] if single else vals_sel,\n",
    "            # only flatten base when single sample; otherwise keep original shape\n",
    "            base_values=float(np.ravel(base_sel)[0]) if (single and np.size(base_sel) >= 1) else base_sel,\n",
    "            # only flatten data when single sample\n",
    "            data=data_sel[0, :] if (data_sel.ndim >= 2 and data_sel.shape[0] == 1) else data_sel,\n",
    "            feature_names=expl.feature_names\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unexpected SHAP values shape: {vals.shape}\")\n",
    "\n",
    "# get mean absolute SHAP values and beeswarm plot for top 20\n",
    "def shap_beeswarm_and_importance(df_use, feature_columns, fold_col, rf_params, out_prefix, demographic_variables=demographic_variables):\n",
    "\n",
    "    linguistic = get_linguistic_features()\n",
    "    acoustic   = get_acoustic_features()\n",
    "    def family_of(c):\n",
    "        if c in linguistic: return \"linguistic\"\n",
    "        if c in acoustic:   return \"acoustic\"\n",
    "        if c in demographic_variables: return \"demographics\"\n",
    "        return \"other\"\n",
    "    superscript_for = {\"linguistic\":\"¹\", \"acoustic\":\"²\", \"demographics\":\"³\", \"other\":\"\"}\n",
    "\n",
    "    X_all = df_use[feature_columns].copy()\n",
    "    y_all = df_use[\"is_low\"].astype(int).to_numpy()\n",
    "    folds = sorted(df_use[fold_col].unique())\n",
    "\n",
    "    vals_list, data_list = [], []\n",
    "\n",
    "    for f in folds:\n",
    "        mask = (df_use[fold_col] == f)\n",
    "        X_tr, y_tr = X_all.loc[~mask], y_all[~mask]\n",
    "        X_te        = X_all.loc[mask]\n",
    "\n",
    "        clf = RandomForestClassifier(**rf_params)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "\n",
    "        # TreeExplainer for SHAP values\n",
    "        explainer = shap.TreeExplainer(clf)  # default model_output=\"raw\"\n",
    "        ex_all = explainer(X_te)\n",
    "\n",
    "        # ensure single-output (class 1)\n",
    "        ex = to_single_output_explanation(ex_all, class_index=1)\n",
    "\n",
    "        vals_list.append(np.array(ex.values))  # [n_te, n_feat]\n",
    "        data_list.append(np.array(ex.data))    # [n_te, n_feat]\n",
    "\n",
    "    V = np.vstack(vals_list)   # [N, F]\n",
    "    D = np.vstack(data_list)   # [N, F]\n",
    "\n",
    "    # label with superscripts for feature-categories\n",
    "    names = list(feature_columns)\n",
    "    labeled = [f\"{n}{superscript_for[family_of(n)]}\" for n in names]\n",
    "\n",
    "    expl = shap.Explanation(values=V,\n",
    "                            base_values=np.mean(V, axis=0),\n",
    "                            data=D,\n",
    "                            feature_names=labeled)\n",
    "\n",
    "    # beeswarm plot for top 20\n",
    "    shap.plots.beeswarm(expl, max_display=20, show=False)\n",
    "    plt.figtext(0.0, 0.01, \"¹ linguistic   ² acoustic   ³ demographics\",\n",
    "                ha=\"left\", va=\"bottom\", fontsize=9)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "    fig = plt.gcf(); ax = plt.gca()\n",
    "    fig.patch.set_facecolor(\"white\"); ax.set_facecolor(\"white\")\n",
    "    plt.savefig(out_prefix + \"_shap_beeswarm.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # save mean absolute SHAP values\n",
    "    mean_abs = np.abs(V).mean(axis=0)\n",
    "    feature_importances = (pd.DataFrame({\"feature\": names, \"mean_abs_shap\": mean_abs})\n",
    "             .sort_values(\"mean_abs_shap\", ascending=False))\n",
    "    feature_importances.to_csv(out_prefix + \"_shap_importance.csv\", index=False)\n",
    "    return feature_importances\n",
    "\n",
    "# plot one individual (low performer) as SHAP waterfall plot\n",
    "def plot_local_waterfall(task, target_score, model_name, subject_id=None,\n",
    "                         outdir=out_dir, max_display=15, random_state=random_state):\n",
    "\n",
    "    rf_params = load_tuned_rf_params()\n",
    "    df = load_data_for(task, target_score)\n",
    "    df, _, _ = compute_norms_and_labels(df, target_score)\n",
    "    model_features = build_feature_sets(df.columns, demographic_variables)\n",
    "    feature_columns = model_features[model_name]\n",
    "\n",
    "    subset = [target_score] + feature_columns\n",
    "    df_use = df.dropna(subset=subset).copy()\n",
    "    X_all = df_use[feature_columns].copy()\n",
    "    y_all = df_use[\"is_low\"].astype(int).to_numpy()\n",
    "\n",
    "    # builds predictions to pick low performer\n",
    "    clf = RandomForestClassifier(**rf_params)\n",
    "    oof_rows = []\n",
    "    for f in sorted(df_use[\"fold\"].unique()):\n",
    "        mask = (df_use[\"fold\"] == f)\n",
    "        X_train, y_train = X_all.loc[~mask], y_all[~mask]\n",
    "        X_test,  y_test  = X_all.loc[mask],  y_all[mask]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "        oof_rows.append(pd.DataFrame({\n",
    "            \"Subject_ID\": df_use.loc[mask, \"Subject_ID\"].values,\n",
    "            \"fold\": f, \"y_true\": y_test, \"y_prob\": y_prob\n",
    "        }))\n",
    "    oof = pd.concat(oof_rows, ignore_index=True)\n",
    "\n",
    "    # choose subject with highest predicted probability for being low performer\n",
    "    if subject_id is None:\n",
    "        lows = oof[oof[\"y_true\"] == 1]\n",
    "        if lows.empty:\n",
    "            raise ValueError(\"No true low performers found in this subset.\")\n",
    "        subject_id = lows.sort_values(\"y_prob\", ascending=False)[\"Subject_ID\"].iloc[0]\n",
    "\n",
    "    row = df_use[df_use[\"Subject_ID\"] == subject_id]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Subject_ID {subject_id} not found.\")\n",
    "    s_fold = int(row[\"fold\"].iloc[0])\n",
    "\n",
    "    # trains model on folds except the subject's fold\n",
    "    train_df = df_use[df_use[\"fold\"] != s_fold].copy()\n",
    "    test_row = row.copy()\n",
    "\n",
    "    X_train = train_df[feature_columns]; y_train = train_df[\"is_low\"].astype(int)\n",
    "    X_test_one = test_row[feature_columns]; y_test_one = int(test_row[\"is_low\"].iloc[0])\n",
    "\n",
    "    # fit local model & compute SHAP\n",
    "    clf_local = RandomForestClassifier(**rf_params)\n",
    "    clf_local.fit(X_train, y_train)\n",
    "\n",
    "    explainer = shap.TreeExplainer(clf_local)\n",
    "    ex_all = explainer(X_test_one)\n",
    "\n",
    "    ex = to_single_output_explanation(ex_all, class_index=1)\n",
    "\n",
    "    # label features with superscripts\n",
    "    superscript_for = {\"linguistic\": \"¹\", \"acoustic\": \"²\", \"demographics\": \"³\", \"other\": \"\"}\n",
    "    linguistic = get_linguistic_features()\n",
    "    acoustic   = get_acoustic_features()\n",
    "    families = {\n",
    "        c: (\"linguistic\" if c in linguistic else\n",
    "            \"acoustic\"   if c in acoustic   else\n",
    "            \"demographics\" if c in demographic_variables else \"other\")\n",
    "        for c in feature_columns\n",
    "    }\n",
    "\n",
    "    # label with superscripts for feature-categories\n",
    "    labeled = [f\"{n}{superscript_for.get(families.get(n,'other'), '')}\" for n in ex.feature_names]\n",
    "    ex_labeled = shap.Explanation(values=ex.values, base_values=ex.base_values, data=ex.data, feature_names=labeled)\n",
    "\n",
    "    prefix = f\"{task}_{target_score}_{model_name}\"\n",
    "    save_prefix = os.path.join(outdir, prefix)\n",
    "\n",
    "    # create waterfall plot\n",
    "    pred_prob = float(clf_local.predict_proba(X_test_one)[:, 1][0])\n",
    "    title = (f\"Local SHAP Waterfall\\n\"\n",
    "             f\"Subject {subject_id} | True label: {y_test_one} | Predicted P(low)={pred_prob:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(ex_labeled, max_display=max_display, show=False)\n",
    "    plt.title(title, fontsize=11)\n",
    "    plt.figtext(0.00, 0.02, \"¹ linguistic   ² acoustic   ³ demographics\",\n",
    "                ha=\"left\", va=\"top\", fontsize=10)\n",
    "    plt.subplots_adjust(left=0.30, right=0.96, bottom=0.15, top=0.85)\n",
    "    out_path = f\"{save_prefix}_waterfall_subject-{subject_id}.png\"\n",
    "    fig = plt.gcf()\n",
    "    ax  = plt.gca()\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    ax.set_facecolor(\"white\")\n",
    "    plt.savefig(out_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"saved waterfall plot:\", out_path)\n",
    "\n",
    "    return {\"subject_id\": subject_id, \"fold\": s_fold, \"pred_prob\": pred_prob, \"path\": out_path}\n"
   ],
   "id": "6ce5d976a6255491",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.458538Z",
     "start_time": "2025-11-12T10:56:05.449163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper to run classification\n",
    "\n",
    "# load data\n",
    "def load_data_for(task, target_score):\n",
    "    demo_use = demographics[[\"Subject_ID\"] + demographic_variables].copy()\n",
    "    df = load_task_dataframe(task_name=task, target=target_score,\n",
    "                             scores=scores, demographics=demo_use)\n",
    "    return df\n",
    "\n",
    "# run classification for one task-score-combination\n",
    "def run_classifier(task, target_score, model_name, outdir=out_dir, baseline_threshold=baseline_threshold, n_boot=n_boot, random_state=random_state):\n",
    "    # use tuned hyperparameters\n",
    "    rf_params = load_tuned_rf_params()\n",
    "\n",
    "    # prepare data\n",
    "    df = load_data_for(task, target_score)\n",
    "    df, mean, std = compute_norms_and_labels(df, target_score) # compute the norms and z-values\n",
    "    model_features = build_feature_sets(df.columns, demographic_variables)\n",
    "    feature_columns = model_features[model_name]\n",
    "\n",
    "    subset = [target_score] + feature_columns\n",
    "    df_use = df.dropna(subset=subset).copy()\n",
    "    X = df_use[feature_columns].copy()\n",
    "    y = df_use[\"is_low\"].to_numpy()\n",
    "\n",
    "    clf = RandomForestClassifier(**rf_params)\n",
    "\n",
    "    # fit once per fold, get y_prob and y_pred from that fitted model\n",
    "    oof_rows = []\n",
    "    for f in sorted(df_use[\"fold\"].unique()):\n",
    "        mask = (df_use[\"fold\"] == f)\n",
    "        X_train, y_train = X.loc[~mask], y[~mask]\n",
    "        X_test,  y_test  = X.loc[mask],  y[mask]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_prob = clf.predict_proba(X_test)[:, 1] # probabilities for ROC/Youden/bootstrap\n",
    "        y_pred = clf.predict(X_test).astype(int) # labels for baseline metrics\n",
    "        oof_rows.append(pd.DataFrame({\n",
    "            \"Subject_ID\": df_use.loc[mask, \"Subject_ID\"].values,\n",
    "            \"fold\": f,\n",
    "            \"y_true\": y_test,\n",
    "            \"y_prob\": y_prob,\n",
    "            \"y_pred\": y_pred\n",
    "        }))\n",
    "    oof = pd.concat(oof_rows, ignore_index=True)\n",
    "\n",
    "    # calculate baseline metrics from y_pred labels, AUROC from y_prob\n",
    "    y_true = oof[\"y_true\"].to_numpy()\n",
    "    y_pred = oof[\"y_pred\"].to_numpy()\n",
    "    y_prob = oof[\"y_prob\"].to_numpy()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    spec = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    bal_acc = 0.5 * (sens + spec)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    auc  = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) == 2 else np.nan\n",
    "    ap = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) == 2 else np.nan\n",
    "\n",
    "    base_metrics = {\n",
    "        \"auroc\": auc,\n",
    "        \"auprc\": ap,\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"npv\": (tn / (tn + fn)) if (tn + fn) > 0 else np.nan,\n",
    "        \"f1\": f1,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "        \"threshold\": np.nan\n",
    "    }\n",
    "\n",
    "    # bootstrap baseline metrics\n",
    "    boot_raw_b, boot_summ_b = bootstrap_classification_metrics(\n",
    "        oof, threshold=baseline_threshold, n_boot=n_boot, ci=0.95, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # for saving\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    prefix = f\"{task}_{target_score}_{model_name}\"\n",
    "\n",
    "    # compute different thresholds and add metrics there\n",
    "    youden = find_best_threshold_youden(y_true, y_prob)\n",
    "    f1_best = find_best_threshold_f1(y_true, y_prob)\n",
    "    rec80 = find_threshold_for_target_recall(y_true, y_prob, target_recall=0.80)\n",
    "\n",
    "    boot_all_raw, boot_all_summ = bootstrap_all_thresholds(\n",
    "        oof, youden_thr=youden[\"best_threshold\"], f1_thr=f1_best[\"best_threshold\"],\n",
    "        rec80_thr=rec80[\"threshold\"], n_boot=n_boot, ci=ci, random_state=random_state,\n",
    "        recall_target=0.80, bootstrap_threshold=\"reselect\"\n",
    "    )\n",
    "\n",
    "    boot_all_summ.to_csv(os.path.join(outdir, f\"{prefix}_bootstrap_metrics_summary.csv\"), index=False)\n",
    "\n",
    "    # save threshold table\n",
    "    thr_df = pd.DataFrame([\n",
    "        {\"task\": task, \"target\": target_score, \"model\": model_name,\n",
    "         \"criterion\": \"max_f1\", **f1_best},\n",
    "        {\"task\": task, \"target\": target_score, \"model\": model_name,\n",
    "         \"criterion\": \"recall>=0.80\", **rec80}\n",
    "    ])\n",
    "    thr_df.to_csv(os.path.join(outdir, f\"{prefix}_pr_thresholds.csv\"), index=False)\n",
    "\n",
    "    # evaluate full metrics at those thresholds\n",
    "    f1_metrics = classification_metrics(y_true, y_prob, threshold=f1_best[\"best_threshold\"])\n",
    "    rec_metrics = classification_metrics(y_true, y_prob, threshold=rec80[\"threshold\"])\n",
    "    youden_metrics = classification_metrics(y_true, y_prob, threshold=youden[\"best_threshold\"])\n",
    "\n",
    "    # combine all metrics into one dataframe\n",
    "    metrics = pd.DataFrame([\n",
    "        {\"task\": task, \"target\": target_score, \"model\": model_name,\n",
    "         \"threshold_type\": \"baseline_predict\", **base_metrics},\n",
    "        {\"task\": task, \"target\": target_score, \"model\": model_name,\n",
    "         \"threshold_type\": \"youdenJ\", **youden_metrics, **youden},\n",
    "        {\"task\": task, \"target\": target_score, \"model\": model_name,\n",
    "         \"threshold_type\": \"max_f1\", **f1_metrics, **f1_best},\n",
    "        {\"task\": task, \"target\": target_score, \"model\": model_name,\n",
    "         \"threshold_type\": \"recall>=0.80\", **rec_metrics, **rec80}\n",
    "    ])\n",
    "\n",
    "    # save metrics based on random forest classifier pred & based on proba with youden's index + precision-recall based ones\n",
    "    metrics.to_csv(os.path.join(outdir, f\"{prefix}_metrics_summary.csv\"), index=False)\n",
    "\n",
    "    # save predictions\n",
    "    oof.to_csv(os.path.join(outdir, f\"{prefix}_oof.csv\"), index=False)\n",
    "\n",
    "    # plots\n",
    "    plot_roc_curve(\n",
    "        oof, save_path=os.path.join(outdir, f\"{prefix}_roc_bootstrapped.png\"),\n",
    "        n_boot=n_boot, random_state=random_state\n",
    "    )\n",
    "    plot_confusion_matrix(\n",
    "        oof,\n",
    "        save_path=os.path.join(outdir, f\"{prefix}_confusion_matrix.png\")\n",
    "    )\n",
    "    plot_pr_curve(\n",
    "        oof,\n",
    "        save_path=os.path.join(outdir, f\"{prefix}_precision-recall.png\")\n",
    "    )\n",
    "    # confusion matrix: recall >= 0.80\n",
    "    plot_confusion_matrix_at_threshold(\n",
    "        oof, rec80[\"threshold\"],\n",
    "        save_path=os.path.join(outdir, f\"{prefix}_cm_recall80.png\"),\n",
    "        normalize=False\n",
    "    )\n",
    "    # confusion matrix: balanced reference (max-F1)\n",
    "    plot_confusion_matrix_at_threshold(\n",
    "        oof, f1_best[\"best_threshold\"],\n",
    "        save_path=os.path.join(outdir, f\"{prefix}_cm_maxF1.png\"),\n",
    "        normalize=False\n",
    "    )\n",
    "    # confusion matrix: Youden-J\n",
    "    plot_confusion_matrix_at_threshold(\n",
    "        oof, youden[\"best_threshold\"],\n",
    "        save_path=os.path.join(outdir, f\"{prefix}_cm_youdenJ.png\"),\n",
    "        normalize=False\n",
    "    )\n",
    "\n",
    "    # SHAP (global) for full model\n",
    "    if model_name == \"full\":\n",
    "        try:\n",
    "            out_prefix = os.path.join(outdir, prefix)\n",
    "            shap_beeswarm_and_importance(\n",
    "                df_use=df_use,\n",
    "                feature_columns=feature_columns,\n",
    "                fold_col=\"fold\",\n",
    "                rf_params=rf_params,\n",
    "                out_prefix=out_prefix,\n",
    "                demographic_variables=demographic_variables\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"SHAP skipped due to error:\", e)\n",
    "\n",
    "    return {\n",
    "        \"oof\": oof,\n",
    "        \"metrics_summary\": metrics,\n",
    "        \"boot_baseline_summary\": boot_summ_b\n",
    "    }\n",
    "\n",
    "# run classificaiton for different models of one task-score combination\n",
    "def run_all_models(task: str, target: str, models=(\"demographics\",\"acoustic\",\"linguistic\",\"acoustic+linguistic\",\"full\")):\n",
    "    results = []\n",
    "    for m in models:\n",
    "        print(f\"{task} - {target} - {m}\")\n",
    "        out = run_classifier(task, target, m)\n",
    "        results.append(out[\"metrics_summary\"].assign(task=task, target=target, model=m))\n",
    "    return pd.concat(results, ignore_index=True)\n"
   ],
   "id": "e3b351528ba3571d",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.473631Z",
     "start_time": "2025-11-12T10:56:05.466454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper for hyperparameter-tuning\n",
    "def classification_hyperparameter_tuning(\n",
    "    n_iter_random=100, refine_with_grid=True, test_half_label=1, random_state=random_state\n",
    "):\n",
    "    outdir = os.path.join(out_dir, \"grid_search\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    task_name = \"picnicScene\"\n",
    "    target = \"SemanticFluencyScore\"\n",
    "    model_name = \"full\"\n",
    "\n",
    "    # features\n",
    "    features_path = os.path.join(GIT_DIRECTORY, \"results\", \"features\", \"filtered\", f\"{task_name}_filtered2.csv\")\n",
    "    X_task = pd.read_csv(features_path)\n",
    "\n",
    "    # scores\n",
    "    scores_all = pd.read_csv(scores_csv)\n",
    "    scores = scores_all[[\"Subject_ID\", target]].dropna(subset=[target]).copy()\n",
    "\n",
    "    # demographics\n",
    "    demo = demographics[[\"Subject_ID\"] + demographic_variables].copy()\n",
    "\n",
    "    # folds for half-split\n",
    "    folds = pd.read_csv(os.path.join(GIT_DIRECTORY, \"data\", \"stratified_folds_grid_search.csv\"))\n",
    "\n",
    "    # merge\n",
    "    df = (X_task.merge(demo, on=\"Subject_ID\", how=\"left\")\n",
    "               .merge(scores, on=\"Subject_ID\", how=\"inner\")\n",
    "               .merge(folds[[\"Subject_ID\",\"fold\"]], on=\"Subject_ID\", how=\"inner\"))\n",
    "\n",
    "    linguistic = get_linguistic_features(); acoustic = get_acoustic_features()\n",
    "    feature_columns = sorted(list(linguistic | acoustic)) + demographic_variables\n",
    "    feature_columns = [c for c in feature_columns if c in df.columns]\n",
    "\n",
    "    df = df.dropna(subset=[target] + feature_columns).reset_index(drop=True)\n",
    "\n",
    "    # split in half\n",
    "    train_df = df[df[\"fold\"] != test_half_label].reset_index(drop=True)\n",
    "    test_df  = df[df[\"fold\"] == test_half_label].reset_index(drop=True)\n",
    "\n",
    "    X_train = train_df[feature_columns].copy()\n",
    "    X_test  = test_df[feature_columns].copy()\n",
    "\n",
    "    # train M & SD for both halves (avoid leakage)\n",
    "    y_train = (((train_df[target] - train_df[target].mean()) / train_df[target].std()) < -1).astype(int).to_numpy()\n",
    "    y_test  = (((test_df[target]  - train_df[target].mean()) / train_df[target].std()) < -1).astype(int).to_numpy()\n",
    "\n",
    "    print(f\"{task_name} | full | train N={len(X_train)}, test N={len(X_test)}, features={len(feature_columns)}\")\n",
    "\n",
    "    # inner CV on train half only\n",
    "    inner_cv = list(KFold(n_splits=5, shuffle=True, random_state=random_state).split(np.arange(len(X_train))))\n",
    "\n",
    "    param_dist = {\n",
    "        \"n_estimators\": randint(100, 1001),\n",
    "        \"max_depth\": [None] + list(range(10, 31)),\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": randint(2, 11),\n",
    "        \"min_samples_leaf\": randint(1, 6),\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"class_weight\": [\"balanced\"]\n",
    "    }\n",
    "\n",
    "    # randomized search first\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter_random,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=inner_cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        random_state=random_state,\n",
    "        refit=True,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_params = rs.best_params_\n",
    "    best_cv = float(rs.best_score_)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    pd.DataFrame(rs.cv_results_).to_csv(os.path.join(outdir, \"randomized_search_cv_results.csv\"), index=False)\n",
    "    print(\"RandomizedSearch best params:\", best_params)\n",
    "    print(\"RandomizedSearch best inner-CV AUROC:\", f\"{best_cv:.3f}\")\n",
    "\n",
    "    final_params = best_params\n",
    "\n",
    "    # grid search after\n",
    "    if refine_with_grid:\n",
    "        bp = best_params\n",
    "        def win(center, radius, low, high, step=1):\n",
    "            if center is None:\n",
    "                return [None]\n",
    "            a = max(low, int(center) - radius)\n",
    "            b = min(high, int(center) + radius)\n",
    "            return list(range(a, b + 1, step)) if a <= b else [int(center)]\n",
    "        n_estimators_step = 50\n",
    "        grid = {\n",
    "            \"n_estimators\": win(bp[\"n_estimators\"], radius=200, low=100, high=1000, step=n_estimators_step),\n",
    "            \"max_depth\": [None] if bp[\"max_depth\"] is None else win(bp[\"max_depth\"], radius=3, low=10, high=30, step=1),\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "            \"min_samples_split\": win(bp[\"min_samples_split\"], radius=1, low=2, high=10, step=1),\n",
    "            \"min_samples_leaf\": win(bp[\"min_samples_leaf\"], radius=1, low=1, high=5, step=1),\n",
    "            \"bootstrap\": [True, False],\n",
    "            \"class_weight\": [\"balanced\"]\n",
    "        }\n",
    "        gs = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "            param_grid=grid,\n",
    "            scoring=\"roc_auc\",\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            refit=True,\n",
    "            return_train_score=False\n",
    "        )\n",
    "        gs.fit(X_train, y_train)\n",
    "        best_params = gs.best_params_\n",
    "        best_cv = float(gs.best_score_)\n",
    "        pd.DataFrame(gs.cv_results_).to_csv(os.path.join(outdir, \"grid_search_cv_results.csv\"), index=False)\n",
    "        print(\"GridSearch best params:\", best_params)\n",
    "        print(\"GridSearch best inner-CV AUROC:\", f\"{best_cv:.3f}\")\n",
    "        final_params = best_params\n",
    "\n",
    "    pd.DataFrame([final_params]).to_csv(os.path.join(outdir, \"best_params_final.csv\"), index=False)\n",
    "    return final_params\n"
   ],
   "id": "70c161dc82cb7090",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:56:05.482840Z",
     "start_time": "2025-11-12T10:56:05.481111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#_ = classification_hyperparameter_tuning(\n",
    "#    n_iter_random=100,\n",
    "#    refine_with_grid=True,\n",
    "#    test_half_label=1,\n",
    "#    random_state=random_state\n",
    "#)"
   ],
   "id": "3c5918dbb3d3421f",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:58:12.009203Z",
     "start_time": "2025-11-12T10:56:05.497055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run for all models and save results\n",
    "results_df = run_all_models(task, target, models=(\"demographics\",\"acoustic\",\"linguistic\",\"acoustic+linguistic\",\"full\"))\n",
    "results_df.tail()"
   ],
   "id": "1c0bf456bd8b03b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picnicScene - PhonemicFluencyScore - demographics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picnicScene - PhonemicFluencyScore - acoustic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picnicScene - PhonemicFluencyScore - linguistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picnicScene - PhonemicFluencyScore - acoustic+linguistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picnicScene - PhonemicFluencyScore - full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "           task                target                model    threshold_type  \\\n",
       "15  picnicScene  PhonemicFluencyScore  acoustic+linguistic      recall>=0.80   \n",
       "16  picnicScene  PhonemicFluencyScore                 full  baseline_predict   \n",
       "17  picnicScene  PhonemicFluencyScore                 full           youdenJ   \n",
       "18  picnicScene  PhonemicFluencyScore                 full            max_f1   \n",
       "19  picnicScene  PhonemicFluencyScore                 full      recall>=0.80   \n",
       "\n",
       "       auroc     auprc  sensitivity  specificity  accuracy  precision  ...  \\\n",
       "15  0.581566       NaN     0.805556     0.239001  0.321827   0.153439  ...   \n",
       "16  0.576637  0.208256     0.013889     0.996433  0.852792   0.400000  ...   \n",
       "17  0.576637       NaN     0.423611     0.741974  0.695431   0.219424  ...   \n",
       "18  0.576637       NaN     0.423611     0.741974  0.695431   0.219424  ...   \n",
       "19  0.576637       NaN     0.805556     0.240190  0.322843   0.153642  ...   \n",
       "\n",
       "    balanced_accuracy       mcc   tp   fp   tn   fn  threshold  \\\n",
       "15           0.522278  0.037266  116  640  201   28   0.138753   \n",
       "16           0.505161  0.051314    2    3  838  142        NaN   \n",
       "17           0.582792  0.129977   61  217  624   83   0.236732   \n",
       "18           0.582792  0.129977   61  217  624   83   0.236732   \n",
       "19           0.522873  0.038203  116  639  202   28   0.141571   \n",
       "\n",
       "    best_threshold         J    recall  \n",
       "15             NaN       NaN  0.805556  \n",
       "16             NaN       NaN       NaN  \n",
       "17        0.236732  0.165585       NaN  \n",
       "18        0.236732       NaN  0.423611  \n",
       "19             NaN       NaN  0.805556  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>target</th>\n",
       "      <th>model</th>\n",
       "      <th>threshold_type</th>\n",
       "      <th>auroc</th>\n",
       "      <th>auprc</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>...</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>mcc</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>threshold</th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>J</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>picnicScene</td>\n",
       "      <td>PhonemicFluencyScore</td>\n",
       "      <td>acoustic+linguistic</td>\n",
       "      <td>recall&gt;=0.80</td>\n",
       "      <td>0.581566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.239001</td>\n",
       "      <td>0.321827</td>\n",
       "      <td>0.153439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522278</td>\n",
       "      <td>0.037266</td>\n",
       "      <td>116</td>\n",
       "      <td>640</td>\n",
       "      <td>201</td>\n",
       "      <td>28</td>\n",
       "      <td>0.138753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>picnicScene</td>\n",
       "      <td>PhonemicFluencyScore</td>\n",
       "      <td>full</td>\n",
       "      <td>baseline_predict</td>\n",
       "      <td>0.576637</td>\n",
       "      <td>0.208256</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.996433</td>\n",
       "      <td>0.852792</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505161</td>\n",
       "      <td>0.051314</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>838</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>picnicScene</td>\n",
       "      <td>PhonemicFluencyScore</td>\n",
       "      <td>full</td>\n",
       "      <td>youdenJ</td>\n",
       "      <td>0.576637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423611</td>\n",
       "      <td>0.741974</td>\n",
       "      <td>0.695431</td>\n",
       "      <td>0.219424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582792</td>\n",
       "      <td>0.129977</td>\n",
       "      <td>61</td>\n",
       "      <td>217</td>\n",
       "      <td>624</td>\n",
       "      <td>83</td>\n",
       "      <td>0.236732</td>\n",
       "      <td>0.236732</td>\n",
       "      <td>0.165585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>picnicScene</td>\n",
       "      <td>PhonemicFluencyScore</td>\n",
       "      <td>full</td>\n",
       "      <td>max_f1</td>\n",
       "      <td>0.576637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423611</td>\n",
       "      <td>0.741974</td>\n",
       "      <td>0.695431</td>\n",
       "      <td>0.219424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582792</td>\n",
       "      <td>0.129977</td>\n",
       "      <td>61</td>\n",
       "      <td>217</td>\n",
       "      <td>624</td>\n",
       "      <td>83</td>\n",
       "      <td>0.236732</td>\n",
       "      <td>0.236732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>picnicScene</td>\n",
       "      <td>PhonemicFluencyScore</td>\n",
       "      <td>full</td>\n",
       "      <td>recall&gt;=0.80</td>\n",
       "      <td>0.576637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.240190</td>\n",
       "      <td>0.322843</td>\n",
       "      <td>0.153642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522873</td>\n",
       "      <td>0.038203</td>\n",
       "      <td>116</td>\n",
       "      <td>639</td>\n",
       "      <td>202</td>\n",
       "      <td>28</td>\n",
       "      <td>0.141571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:58:17.216267Z",
     "start_time": "2025-11-12T10:58:12.072766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add waterfall plot\n",
    "waterfall_plot = plot_local_waterfall(task, target, model)\n",
    "waterfall_plot"
   ],
   "id": "6faee9fd1ac0f3b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved waterfall plot: /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/classification/picnicScene_PhonemicFluencyScore_full_waterfall_subject-922.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject_id': np.int64(922),\n",
       " 'fold': 4,\n",
       " 'pred_prob': 0.5618482743255889,\n",
       " 'path': '/Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/classification/picnicScene_PhonemicFluencyScore_full_waterfall_subject-922.png'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:58:17.242080Z",
     "start_time": "2025-11-12T10:58:17.234746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def plot_multi_roc_overlay(\n",
    "    task, target, models, outdir,\n",
    "    n_boot=1000, random_state=42,\n",
    "    use_saved_oof=True, oof_dict=None,\n",
    "    file_prefix_template=\"{task}_{target}_{model}_oof.csv\",\n",
    "    save_name=None,\n",
    "    show_ci=\"none\", # \"none\" / \"all\" / \"one\"\n",
    "    ci_model=\"full\", # which model gets CI if show_ci==\"one\"\n",
    "    legend_metric=\"none\" # \"none\" / \"mean\"  -> \"mean\" = add AUROC values\n",
    "):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    fpr_grid = np.linspace(0, 1, 101)\n",
    "\n",
    "    # display model names for legend\n",
    "    label_map = {\n",
    "        \"full\": \"full\",\n",
    "        \"acoustic+linguistic\": \"acoustic + linguistic\",\n",
    "        \"linguistic\": \"linguistic\",\n",
    "        \"acoustic\": \"acoustic\",\n",
    "        \"demographics\": \"demographic\"\n",
    "    }\n",
    "\n",
    "    # load OOFs\n",
    "    data = {}\n",
    "    for m in models:\n",
    "        if use_saved_oof:\n",
    "            path = os.path.join(outdir, file_prefix_template.format(task=task, target=target, model=m))\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            if oof_dict is None or m not in oof_dict:\n",
    "                raise ValueError(f\"Missing OOF data for model '{m}'.\")\n",
    "            df = oof_dict[m].copy()\n",
    "        df = df[[\"Subject_ID\",\"y_true\",\"y_prob\"]].dropna().copy()\n",
    "        df[\"Subject_ID\"] = df[\"Subject_ID\"].astype(str)\n",
    "        data[m] = df\n",
    "\n",
    "    # align subjects (intersection)\n",
    "    common = set.intersection(*(set(d[\"Subject_ID\"]) for d in data.values()))\n",
    "    for m in models:\n",
    "        data[m] = (data[m][data[m][\"Subject_ID\"].isin(common)]\n",
    "                   .sort_values(\"Subject_ID\").reset_index(drop=True))\n",
    "    N = len(next(iter(data.values())))\n",
    "    if N == 0:\n",
    "        raise ValueError(\"After alignment, there are 0 samples.\")\n",
    "\n",
    "    # bootstrap ROC per model\n",
    "    results = {}\n",
    "    idxs = rng.randint(0, N, size=(n_boot, N))\n",
    "    for m in models:\n",
    "        dfm = data[m]\n",
    "        y = dfm[\"y_true\"].to_numpy().astype(int)\n",
    "        p = dfm[\"y_prob\"].to_numpy().astype(float)\n",
    "\n",
    "        tprs = np.empty((n_boot, fpr_grid.size))\n",
    "        aucs = np.empty(n_boot)\n",
    "        for b in range(n_boot):\n",
    "            ii = idxs[b]\n",
    "            yb, pb = y[ii], p[ii]\n",
    "            if np.unique(yb).size < 2:\n",
    "                tprs[b, :] = np.nan\n",
    "                aucs[b] = np.nan\n",
    "                continue\n",
    "            fpr_b, tpr_b, _ = roc_curve(yb, pb)\n",
    "            aucs[b] = roc_auc_score(yb, pb)\n",
    "            tprs[b, :] = np.interp(fpr_grid, fpr_b, tpr_b, left=0.0, right=1.0)\n",
    "\n",
    "        results[m] = {\n",
    "            \"tpr_mean\": np.nanmean(tprs, axis=0),\n",
    "            \"tpr_lo\":   np.nanquantile(tprs, 0.025, axis=0),\n",
    "            \"tpr_hi\":   np.nanquantile(tprs, 0.975, axis=0),\n",
    "            \"auc_mean\": float(np.nanmean(aucs)),\n",
    "        }\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6.8, 5.6))\n",
    "    for m in models:\n",
    "        r = results[m]\n",
    "        lw = 1.2\n",
    "        z  = 2   if m != \"full\" else 3  # full line on top\n",
    "        display_name = label_map.get(m, m)\n",
    "\n",
    "        if legend_metric == \"mean\":\n",
    "            label = f\"{display_name} | AUROC = {r['auc_mean']:.3f}\"\n",
    "        else:\n",
    "            label = display_name\n",
    "\n",
    "        # CI (behind line)\n",
    "        if show_ci == \"all\" or (show_ci == \"one\" and m == ci_model):\n",
    "            plt.fill_between(\n",
    "                fpr_grid, r[\"tpr_lo\"], r[\"tpr_hi\"],\n",
    "                alpha=0.15, color=colors.get(m, None), zorder=1\n",
    "            )\n",
    "\n",
    "        # mean ROC line\n",
    "        plt.plot(\n",
    "            fpr_grid, r[\"tpr_mean\"],\n",
    "            label=label, linewidth=lw, color=colors.get(m, None), zorder=z\n",
    "        )\n",
    "\n",
    "    # chance\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\", linewidth=1,\n",
    "             color=\"gray\", label=\"Chance\")\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize=12)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, fancybox=True, framealpha=0.9)\n",
    "    plt.grid(alpha=0.15, linestyle='--')\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_name is None:\n",
    "        save_name = f\"{task}_{target}_all_ROC_curves.png\"\n",
    "    save_path = os.path.join(outdir, save_name)\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", save_path)\n",
    "    return results\n"
   ],
   "id": "e7d58207c9cbec9a",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:58:21.062897Z",
     "start_time": "2025-11-12T10:58:17.256936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "overlay = plot_multi_roc_overlay(\n",
    "    task,\n",
    "    target,\n",
    "    models=(\"demographics\", \"acoustic\", \"linguistic\", \"acoustic+linguistic\", \"full\"),\n",
    "    outdir=os.path.join(GIT_DIRECTORY, \"results\", \"classification\"),\n",
    "    show_ci=\"one\",\n",
    "    ci_model=\"full\",\n",
    "    legend_metric=\"mean\"   # <— key change\n",
    ")\n"
   ],
   "id": "3bf9bb464847f1ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/classification/picnicScene_PhonemicFluencyScore_all_ROC_curves.png\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:58:21.080820Z",
     "start_time": "2025-11-12T10:58:21.071696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_multi_pr_overlay(\n",
    "    task,\n",
    "    target,\n",
    "    models,\n",
    "    outdir,\n",
    "    n_boot=1000,\n",
    "    ci=0.95,\n",
    "    random_state=42,\n",
    "    use_saved_oof=True,\n",
    "    oof_dict=None,\n",
    "    file_prefix_template=\"{task}_{target}_{model}_oof.csv\",\n",
    "    save_name=None,\n",
    "    legend_metric=\"mean\", # \"none\" / \"mean\" (AUPRC)\n",
    "    show_f1_for=\"full\", # \"all\" / model name / None\n",
    "):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    label_map = {\n",
    "        \"full\": \"full\",\n",
    "        \"acoustic+linguistic\": \"acoustic + linguistic\",\n",
    "        \"linguistic\": \"linguistic\",\n",
    "        \"acoustic\": \"acoustic\",\n",
    "        \"demographics\": \"demographic\"\n",
    "    }\n",
    "\n",
    "    # load OOFs\n",
    "    data = {}\n",
    "    for m in models:\n",
    "        if use_saved_oof:\n",
    "            path = os.path.join(\n",
    "                outdir,\n",
    "                file_prefix_template.format(task=task, target=target, model=m)\n",
    "            )\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            if oof_dict is None or m not in oof_dict:\n",
    "                raise ValueError(f\"Missing OOF data for model '{m}'.\")\n",
    "            df = oof_dict[m].copy()\n",
    "\n",
    "        df = df[[\"Subject_ID\", \"y_true\", \"y_prob\"]].dropna().copy()\n",
    "        df[\"Subject_ID\"] = df[\"Subject_ID\"].astype(str)\n",
    "        data[m] = df\n",
    "\n",
    "    # align on common subjects\n",
    "    common = set.intersection(*(set(d[\"Subject_ID\"]) for d in data.values()))\n",
    "    if not common:\n",
    "        raise ValueError(\"After alignment, there are 0 common samples across models.\")\n",
    "\n",
    "    for m in models:\n",
    "        data[m] = (\n",
    "            data[m][data[m][\"Subject_ID\"].isin(common)]\n",
    "            .sort_values(\"Subject_ID\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    N = len(next(iter(data.values())))\n",
    "    if N == 0:\n",
    "        raise ValueError(\"No samples after alignment.\")\n",
    "\n",
    "    # shared bootstrap indices for AUPRC CIs\n",
    "    idxs = bootstrap_index_generator(N, n_boot, random_state)\n",
    "    alpha = 1.0 - ci\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # PR + bootstrapped AUPRC + max-F1 for each model\n",
    "    for m in models:\n",
    "        dfm = data[m]\n",
    "        y = dfm[\"y_true\"].to_numpy().astype(int)\n",
    "        p = dfm[\"y_prob\"].to_numpy().astype(float)\n",
    "\n",
    "        # full-data PR curve\n",
    "        pr_prec, pr_rec, _ = precision_recall_curve(y, p, pos_label=1)\n",
    "        ap = average_precision_score(y, p)\n",
    "\n",
    "        # F1-score\n",
    "        f1_info = find_best_threshold_f1(y, p)\n",
    "\n",
    "        # bootstrap AUPRC\n",
    "        auprcs = np.empty(n_boot, dtype=float)\n",
    "        for b in range(n_boot):\n",
    "            ii = idxs[b]\n",
    "            yb, pb = y[ii], p[ii]\n",
    "            if np.unique(yb).size < 2:\n",
    "                auprcs[b] = np.nan\n",
    "            else:\n",
    "                auprcs[b] = average_precision_score(yb, pb)\n",
    "\n",
    "        ci_low = np.nanquantile(auprcs, alpha / 2.0)\n",
    "        ci_high = np.nanquantile(auprcs, 1.0 - alpha / 2.0)\n",
    "\n",
    "        results[m] = {\n",
    "            \"precision\": pr_prec,\n",
    "            \"recall\": pr_rec,\n",
    "            \"auprc\": float(ap),\n",
    "            \"auprc_ci_low\": float(ci_low),\n",
    "            \"auprc_ci_high\": float(ci_high),\n",
    "            \"f1_info\": f1_info,\n",
    "        }\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(6.8, 5.6))\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    any_m = models[0]\n",
    "    prevalence = data[any_m][\"y_true\"].mean()\n",
    "\n",
    "    f1_value = None\n",
    "\n",
    "    # draw PR curves + F1 marker\n",
    "    for m in models:\n",
    "        r = results[m]\n",
    "        display_name = label_map.get(m, m)\n",
    "        col = colors.get(m, None)\n",
    "        lw = 1.2\n",
    "        z = 2 if m != \"full\" else 3\n",
    "\n",
    "        if legend_metric == \"mean\":\n",
    "            label = f\"{display_name} | AUPRC = {r['auprc']:.3f}\"\n",
    "        else:\n",
    "            label = display_name\n",
    "\n",
    "        ax.step(\n",
    "            r[\"recall\"],\n",
    "            r[\"precision\"],\n",
    "            where=\"post\",\n",
    "            linewidth=lw,\n",
    "            color=col,\n",
    "            label=label,\n",
    "            zorder=z,\n",
    "        )\n",
    "\n",
    "        # mark F1 point for chosen model(s), but no separate legend yet\n",
    "        if show_f1_for in (\"all\", m):\n",
    "            fi = r[\"f1_info\"]\n",
    "            ax.scatter(\n",
    "                [fi[\"recall\"]],\n",
    "                [fi[\"precision\"]],\n",
    "                s=36,\n",
    "                facecolors=\"white\",\n",
    "                edgecolors=col if col is not None else \"black\",\n",
    "                zorder=z + 1,\n",
    "            )\n",
    "            if m == show_f1_for:\n",
    "                f1_value = fi[\"f1\"]\n",
    "\n",
    "    # baseline\n",
    "    baseline_label = f\"Baseline (prevalence = {prevalence:.2f})\"\n",
    "    ax.hlines(\n",
    "        prevalence,\n",
    "        0.0,\n",
    "        1.0,\n",
    "        linestyles=\"--\",\n",
    "        linewidth=1,\n",
    "        color=\"gray\",\n",
    "        label=baseline_label,\n",
    "    )\n",
    "\n",
    "    # base legend entries so far\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # add extra legend entry for F1 below baseline entry\n",
    "    if show_f1_for in models and f1_value is not None:\n",
    "        f1_marker = plt.Line2D(\n",
    "            [0], [0],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"None\",\n",
    "            markerfacecolor=\"white\",\n",
    "            markeredgecolor=colors.get(show_f1_for, \"black\"),\n",
    "            markersize=6,\n",
    "        )\n",
    "        f1_label = f\"max F1 ({show_f1_for} model) = {f1_value:.3f}\"\n",
    "        handles.append(f1_marker)\n",
    "        labels.append(f1_label)\n",
    "\n",
    "    ax.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        fancybox=True,\n",
    "        framealpha=0.9,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Recall\", fontsize=12)\n",
    "    ax.set_ylabel(\"Precision\", fontsize=12)\n",
    "    ax.grid(alpha=0.15, linestyle='--')\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_name is None:\n",
    "        save_name = f\"{task}_{target}_all_PR_curves.png\"\n",
    "\n",
    "    save_path = os.path.join(outdir, save_name)\n",
    "    fig.savefig(save_path, dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"Saved:\", save_path)\n",
    "    return results\n"
   ],
   "id": "75a46b303db2a4a5",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T10:58:22.866309Z",
     "start_time": "2025-11-12T10:58:21.106844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pr_overlay = plot_multi_pr_overlay(\n",
    "    task,\n",
    "    target,\n",
    "    models=(\"demographics\", \"acoustic\", \"linguistic\", \"acoustic+linguistic\", \"full\"),\n",
    "    outdir=os.path.join(GIT_DIRECTORY, \"results\", \"classification\"),\n",
    "    legend_metric=\"mean\",\n",
    "    show_f1_for=\"full\"\n",
    ")\n"
   ],
   "id": "3b66166e099088cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n",
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n",
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n",
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n",
      "/var/folders/gp/zgb_1js56gq706y7rfrpmmvh0000gn/T/ipykernel_61141/3416494649.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  f1 = np.where((p + r) > 0, 2 * p * r / (p + r), 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/gilanorup/Desktop/Studium/MSc/MA/code/masters_thesis_gn/results/classification/picnicScene_PhonemicFluencyScore_all_PR_curves.png\n"
     ]
    }
   ],
   "execution_count": 170
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
